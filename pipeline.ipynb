{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sing2Ani Training Pipeline\n",
    "\n",
    "### Dataset Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soundfile\n",
      "cuda:0\n",
      "2.0.1+cu118\n",
      "2.0.2+cu118\n",
      "----------\n",
      "Source: ./sampledata/Training/Audio/8-8-2023 3-15-42 PM.wav\n",
      "----------\n",
      " - sample_rate: 48000\n",
      " - num_channels: 1\n",
      " - num_frames: 86400000\n",
      " - bits_per_sample: 16\n",
      " - encoding: PCM_S\n",
      " - duration: 1800.0s\n",
      " - num_blendshape_frames: 95449\n",
      " - num_bone_frames: 95449\n",
      " - blendshape fps: 53.02722222222222\n",
      " - bone fps: 53.02722222222222\n",
      " - frames per blendshape: 905.195444687739\n",
      " - frames per bone: 905.195444687739\n",
      " - seconds per blendshape: 0.018858238430994562s\n",
      " - seconds per bone: 0.018858238430994562s\n",
      "\n",
      "MFCC with\n",
      " - number of mfcc: 39\n",
      " - number of mels: 78\n",
      " - number of fft: 56\n",
      " - window length: 56\n",
      " - hop length: 28\n",
      " - number of mfcc frames: 3085715\n",
      "\n",
      "----------\n",
      "Source: ./sampledata/Training/Audio/7-17-2023 4-58-09 PM.wav\n",
      "----------\n",
      " - sample_rate: 48000\n",
      " - num_channels: 1\n",
      " - num_frames: 2880000\n",
      " - bits_per_sample: 16\n",
      " - encoding: PCM_S\n",
      " - duration: 60.0s\n",
      " - num_blendshape_frames: 539\n",
      " - num_bone_frames: 539\n",
      " - blendshape fps: 8.983333333333333\n",
      " - bone fps: 8.983333333333333\n",
      " - frames per blendshape: 5343.228200371058\n",
      " - frames per bone: 5343.228200371058\n",
      " - seconds per blendshape: 0.11131725417439703s\n",
      " - seconds per bone: 0.11131725417439703s\n",
      "\n",
      "MFCC with\n",
      " - number of mfcc: 39\n",
      " - number of mels: 78\n",
      " - number of fft: 333\n",
      " - window length: 333\n",
      " - hop length: 166\n",
      " - number of mfcc frames: 17350\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pydub import AudioSegment\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "import torchaudio.functional as F\n",
    "import torchaudio.transforms as T\n",
    "import torch.nn as nn\n",
    "from torchnlp.nn import Attention\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "print(torchaudio.get_audio_backend())\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchaudio.__version__)\n",
    "\n",
    "import librosa\n",
    "\n",
    "from IPython.display import Audio\n",
    "from torchaudio.utils import download_asset\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()   # interactive mode\n",
    "\n",
    "TRAINING_DATA_PATH = \"./sampledata/Training\" # \"C:/Users/Kevin/AppData/LocalLow/kevinjycui/Training\"\n",
    "TESTING_DATA_PATH = \"./sampledata/Testing\" # \"C:/Users/Kevin/AppData/LocalLow/kevinjycui/Testing\"\n",
    "\n",
    "BLENDSHAPE_PARAMS = [\"A\", \"Angry\", \"Blink\", \"Blink_L\", \"Blink_R\", \"E\", \"Fun\", \"I\", \"Joy\", \"LookDown\", \"LookLeft\", \"LookRight\", \"LookUp\", \"Neutral\", \"O\", \"Sorrow\", \"Surprised\", \"U\"]\n",
    "BONE_PARAMS = [\"Chest\", \"Head\", \"Hips\", \"LeftEye\", \"LeftFoot\", \"LeftHand\", \"LeftIndexDistal\", \"LeftIndexIntermediate\", \"LeftIndexProximal\", \"LeftLittleDistal\", \"LeftLittleIntermediate\", \"LeftLittleProximal\", \"LeftLowerArm\", \"LeftLowerLeg\", \"LeftMiddleDistal\", \"LeftMiddleIntermediate\", \"LeftMiddleProximal\", \"LeftRingDistal\", \"LeftRingIntermediate\", \"LeftRingProximal\", \"LeftShoulder\", \"LeftThumbDistal\", \"LeftThumbIntermediate\", \"LeftThumbProximal\", \"LeftToes\", \"LeftUpperArm\", \"LeftUpperLeg\", \"Neck\", \"RightEye\", \"RightFoot\", \"RightHand\", \"RightIndexDistal\", \"RightIndexIntermediate\", \"RightIndexProximal\", \"RightLittleDistal\", \"RightLittleIntermediate\", \"RightLittleProximal\", \"RightLowerArm\", \"RightLowerLeg\", \"RightMiddleDistal\", \"RightMiddleIntermediate\", \"RightMiddleProximal\", \"RightRingDistal\", \"RightRingIntermediate\", \"RightRingProximal\", \"RightShoulder\", \"RightThumbDistal\", \"RightThumbIntermediate\", \"RightThumbProximal\", \"RightToes\", \"RightUpperArm\", \"RightUpperLeg\", \"Spine\", \"UpperChest\"]\n",
    "\n",
    "def print_metadata(metadata, blendshapes, bones, src=None):\n",
    "    if src:\n",
    "        print(\"-\" * 10)\n",
    "        print(\"Source:\", src)\n",
    "        print(\"-\" * 10)\n",
    "    print(\" - sample_rate:\", metadata.sample_rate)\n",
    "    print(\" - num_channels:\", metadata.num_channels)\n",
    "    print(\" - num_frames:\", metadata.num_frames)\n",
    "    print(\" - bits_per_sample:\", metadata.bits_per_sample)\n",
    "    print(\" - encoding:\", metadata.encoding)\n",
    "    duration = metadata.num_frames / metadata.sample_rate\n",
    "    print(\" - duration:\", duration, end='s\\n')\n",
    "    print(\" - num_blendshape_frames:\", len(blendshapes))\n",
    "    print(\" - num_bone_frames:\", len(bones))\n",
    "    print(\" - blendshape fps:\", len(blendshapes) / duration)\n",
    "    print(\" - bone fps:\", len(bones) / duration)\n",
    "    frames_per_blendshape = metadata.num_frames / len(blendshapes)\n",
    "    frames_per_bone = metadata.num_frames / len(bones)\n",
    "    print(\" - frames per blendshape:\", frames_per_blendshape)\n",
    "    print(\" - frames per bone:\", frames_per_bone)\n",
    "    seconds_per_blendshape = duration / len(blendshapes)\n",
    "    seconds_per_bone = duration / len(bones)\n",
    "    print(\" - seconds per blendshape:\", seconds_per_blendshape, end='s\\n')\n",
    "    print(\" - seconds per bone:\", seconds_per_bone, end='s\\n')\n",
    "    assert frames_per_blendshape == frames_per_bone and seconds_per_blendshape == seconds_per_bone\n",
    "    print()\n",
    "\n",
    "def plot_waveform(waveform, sr, title=\"Waveform\"):\n",
    "    waveform = waveform.numpy()\n",
    "\n",
    "    num_channels, num_frames = waveform.shape\n",
    "    time_axis = torch.arange(0, num_frames) / sr\n",
    "\n",
    "    figure, axes = plt.subplots(num_channels, 1)\n",
    "    if num_channels == 1:\n",
    "        axes.plot(time_axis, waveform[0], linewidth=1)\n",
    "        axes.grid(True)\n",
    "    else:\n",
    "        for axis in axes:\n",
    "            axis.plot(time_axis, waveform[0], linewidth=1)\n",
    "            axis.grid(True)\n",
    "    figure.suptitle(title)\n",
    "\n",
    "def plot_spectrogram(specgram, title=None, ylabel=\"freq_bin\"):\n",
    "    fig, axs = plt.subplots(1, 1)\n",
    "    axs.set_title(title or \"Spectrogram (db)\")\n",
    "    axs.set_ylabel(ylabel)\n",
    "    axs.set_xlabel(\"frame\")\n",
    "    im = axs.imshow(librosa.power_to_db(specgram), origin=\"lower\", aspect=\"auto\")\n",
    "    fig.colorbar(im, ax=axs)\n",
    "\n",
    "class VRMParamsDataset(Dataset):\n",
    "    \"\"\" VRM Parameter Dataset \"\"\"\n",
    "\n",
    "    n_mfcc = 39\n",
    "    n_vrmframes = 64\n",
    "\n",
    "    def __init__(self, filename, DATA_PATH=TRAINING_DATA_PATH, effects=None):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            filename (string): Path to wav/csv files.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.name = Path(filename).stem\n",
    "        self.DATA_PATH = DATA_PATH\n",
    "\n",
    "        audio_path = self.DATA_PATH + f\"/Audio/{self.name}.wav\"\n",
    "\n",
    "        self.blendshapes = pd.read_csv(DATA_PATH + \"/Blendshapes/\" + self.name + \".csv\")\n",
    "        self.bones = pd.read_csv(DATA_PATH + \"/Bones/\" + self.name + \".csv\")\n",
    "\n",
    "        assert len(self.blendshapes) == len(self.bones)\n",
    "\n",
    "        metadata = torchaudio.info(audio_path)\n",
    "        print_metadata(metadata, self.blendshapes, self.bones, src=audio_path)\n",
    "\n",
    "        self.SPEECH_WAVEFORM, self.SAMPLE_RATE = torchaudio.load(audio_path)\n",
    "        self.SPEECH_WAVEFORM = torch.mean(self.SPEECH_WAVEFORM, dim=0).unsqueeze(0)\n",
    "\n",
    "        self.STATIC_FRAME = (metadata.num_frames / self.SAMPLE_RATE) / len(self.blendshapes)\n",
    "\n",
    "        for idx in range(len(self.blendshapes)):\n",
    "            approx_time = (idx + 1) * self.STATIC_FRAME\n",
    "            self.blendshapes.iloc[idx, 0] = approx_time\n",
    "            self.bones.iloc[idx, 0] = approx_time\n",
    "\n",
    "        if effects:\n",
    "            self.SPEECH_WAVEFORM, self.SAMPLE_RATE = torchaudio.sox_effects.apply_effects_tensor(self.SPEECH_WAVEFORM, self.SAMPLE_RATE, effects)\n",
    "\n",
    "        def mfcc_transform(n_mfcc, n_mels, n_fft, win_length, hop_length):\n",
    "            print(\"MFCC with\")\n",
    "            print(\" - number of mfcc:\", n_mfcc)\n",
    "            print(\" - number of mels:\", n_mels)\n",
    "            print(\" - number of fft:\", n_fft)\n",
    "            print(\" - window length:\", win_length)\n",
    "            print(\" - hop length:\", hop_length)\n",
    "            return T.MFCC(\n",
    "                sample_rate=self.SAMPLE_RATE,\n",
    "                n_mfcc=n_mfcc,\n",
    "                melkwargs={\n",
    "                    \"n_fft\": n_fft,\n",
    "                    \"n_mels\": n_mels,\n",
    "                    \"win_length\": win_length,\n",
    "                    \"hop_length\": hop_length,\n",
    "                    \"window_fn\": torch.hann_window\n",
    "                },\n",
    "            )\n",
    "        \n",
    "        n_mfcc = self.n_mfcc\n",
    "        n_mels = n_mfcc * 2\n",
    "        n_fft = int(self.STATIC_FRAME * self.SAMPLE_RATE) // 16\n",
    "        win_length = n_fft\n",
    "        hop_length = n_fft // 2\n",
    "\n",
    "        self.mfcc = mfcc_transform(n_mfcc, n_mels, n_fft, win_length, hop_length)(self.SPEECH_WAVEFORM)\n",
    "        self.hop_length = hop_length\n",
    "        print(\" - number of mfcc frames:\", len(self.mfcc[0][0]))\n",
    "        print()\n",
    "\n",
    "    def input_dim(self):\n",
    "        return self.n_mfcc * self.n_vrmframes\n",
    "    \n",
    "    def output_dim(self):\n",
    "        return len(self.blendshapes.iloc[0]) + len(self.bones.iloc[0]) - 2\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.blendshapes)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        # Number of frames before and after timestamp to get\n",
    "        n_vrmframes_half = self.n_vrmframes // 2\n",
    "\n",
    "        mfcc_index = min((idx + 1) * n_vrmframes_half, self.mfcc.shape[2] - n_vrmframes_half - 1)\n",
    "        mfcc_frame = torch.zeros(self.n_mfcc, self.n_vrmframes)\n",
    "        # print(idx, mfcc_index, n_vrmframes_half)\n",
    "        mfcc_frame[:] = self.mfcc[0,:,mfcc_index - n_vrmframes_half:mfcc_index + n_vrmframes_half]\n",
    "                \n",
    "        time_window = self.blendshapes.iloc[idx, 0]\n",
    "\n",
    "        blendshape_params = self.blendshapes.iloc[idx, 1:]\n",
    "        blendshape_params = np.asarray(blendshape_params)\n",
    "        blendshape_params = blendshape_params.astype('float')\n",
    "\n",
    "        assert time_window == self.bones.iloc[idx, 0]\n",
    "\n",
    "        bone_params = self.bones.iloc[idx, 1:]\n",
    "        bone_params = np.asarray(bone_params)\n",
    "        bone_params = bone_params.astype('float')\n",
    "\n",
    "        return mfcc_frame, torch.Tensor(np.concatenate((blendshape_params, bone_params)))\n",
    "    \n",
    "\n",
    "effect = [[\"sinc\", \"300-3k\"]]\n",
    "effect = None # Windows\n",
    "\n",
    "train_file = \"8-8-2023 3-15-42 PM\"\n",
    "\n",
    "train = VRMParamsDataset(train_file, TRAINING_DATA_PATH, effect)\n",
    "test = VRMParamsDataset(\"7-17-2023 4-58-09 PM\", TRAINING_DATA_PATH, effect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VISUAL = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play audio of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (VISUAL):\n",
    "    Audio(train.SPEECH_WAVEFORM, rate=train.SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play audio of testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (VISUAL):\n",
    "    Audio(test.SPEECH_WAVEFORM, rate=test.SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Waveform of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (VISUAL):\n",
    "    plot_waveform(train.SPEECH_WAVEFORM, train.SAMPLE_RATE, title=\"Training audio\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([39, 3085715])\n"
     ]
    }
   ],
   "source": [
    "print(train.mfcc[0].shape)\n",
    "if (VISUAL):\n",
    "    plot_spectrogram(train.mfcc[0])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Waveform of testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (VISUAL):\n",
    "    plot_waveform(test.SPEECH_WAVEFORM, test.SAMPLE_RATE, title=\"Testing audio\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([39, 17350])\n"
     ]
    }
   ],
   "source": [
    "print(test.mfcc[0].shape)\n",
    "if (VISUAL):\n",
    "    plot_spectrogram(test.mfcc[0])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([39, 64]), torch.Size([396]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 200\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "assert train.input_dim() == test.input_dim()\n",
    "assert train.output_dim() == test.output_dim()\n",
    "\n",
    "_x, _y = train[0]\n",
    "_x.shape, _y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VRMLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, huber_weight=1.0, smooth_weight=1.0):\n",
    "        super(VRMLoss, self).__init__()\n",
    "        self.huber_weight = huber_weight\n",
    "        self.smooth_weight = smooth_weight\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        prev = None\n",
    "        huber = 0.\n",
    "        smooth = 0.\n",
    "        for sample in range(len(target)):\n",
    "            huber += nn.HuberLoss()(output[sample], target[sample])\n",
    "            smooth += 0 if prev is None else 1. - nn.CosineSimilarity(dim=0)(output[sample], prev).mean()\n",
    "            prev = output[sample]\n",
    "        return (self.huber_weight * huber + self.smooth_weight * smooth) / len(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, batch_size, output_dim, num_layers, p=0.5):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            input_dim: Input layer dimension\n",
    "            hidden_dim: Hidden layer dimension\n",
    "            batch_size: Batch size of data\n",
    "            output_dim: Output layer dimension\n",
    "            num_layers: Number of layers\n",
    "            p: Dropout\n",
    "        \"\"\"\n",
    "        super(BiLSTM, self).__init__()\n",
    "\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.init_linear = nn.Linear(self.input_dim, self.input_dim)\n",
    "        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, self.num_layers, batch_first=True, bidirectional=True, dropout=p)\n",
    "        \n",
    "        self.linear_hidden = nn.Linear(self.hidden_dim * 2 * self.num_layers, self.hidden_dim)\n",
    "        \n",
    "        self.energy = nn.Linear(self.hidden_dim*3, 1)\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear = nn.Linear(self.hidden_dim*4, output_dim)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return (torch.randn(self.num_layers, self.batch_size, self.hidden_dim),\n",
    "                torch.randn(self.num_layers, self.batch_size, self.hidden_dim))\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = self.dropout(input)\n",
    "        \n",
    "        linear_input = self.init_linear(input)\n",
    "        \n",
    "        lstm_out, (hidden, cell) = self.lstm(linear_input)\n",
    "        \n",
    "        hidden = self.linear_hidden(hidden.reshape(1, -1, self.hidden_dim * 2 * self.num_layers)).permute(1, 0, 2)\n",
    "\n",
    "        attn = self.softmax(self.relu(self.energy(torch.cat((hidden, lstm_out), dim=2))))\n",
    "        context = torch.bmm(attn, lstm_out).permute(1, 0, 2)\n",
    "        \n",
    "        y_pred = self.linear(torch.cat(context, lstm_out)).squeeze()\n",
    "        return y_pred\n",
    "\n",
    "n_epochs = 500\n",
    "lr = 0.0001\n",
    "lstm_input_size = train.input_dim()\n",
    "hidden_state_size = 1024\n",
    "num_sequence_layers = 3\n",
    "output_dim = train.output_dim()\n",
    "save_interval = 22\n",
    "\n",
    "model = BiLSTM(lstm_input_size, hidden_state_size, batch_size, output_dim, num_sequence_layers)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTM(\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (init_linear): Linear(in_features=2496, out_features=2496, bias=True)\n",
       "  (lstm): LSTM(2496, 1024, num_layers=3, batch_first=True, dropout=0.5, bidirectional=True)\n",
       "  (linear_hidden): Linear(in_features=6144, out_features=1024, bias=True)\n",
       "  (energy): Linear(in_features=3072, out_features=1, bias=True)\n",
       "  (softmax): Softmax(dim=0)\n",
       "  (relu): ReLU()\n",
       "  (linear): Linear(in_features=2048, out_features=396, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "huber_weight = 1.0\n",
    "smooth_weight = 1.0\n",
    "loss_fn = VRMLoss(huber_weight, smooth_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, name, train_file, batch_size, epochs, lr, hidden_state_size, num_sequence_layers, huber_weight, smooth_weight):\n",
    "    torch.save(model.state_dict(), 'models/{}.pt'.format(name))\n",
    "    with open('models/meta-{}.txt'.format(name), 'w') as f:\n",
    "        metastring = 'Train file: {}\\nBatch size: {}\\nNumber of epochs: {}\\nLearning rate: {}\\nHidden state size: {}\\nNumber of LSTM layers: {}\\nHuber weight: {}\\nSmooth weight: {}'\n",
    "        f.write(metastring.format(train_file, batch_size, epochs, lr, hidden_state_size, num_sequence_layers, huber_weight, smooth_weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cat() received an invalid combination of arguments - got (Tensor, Tensor), but expected one of:\n * (tuple of Tensors tensors, int dim, *, Tensor out)\n * (tuple of Tensors tensors, name dim, *, Tensor out)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m mfcc \u001b[38;5;241m=\u001b[39m mfcc\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, lstm_input_size)\n\u001b[0;32m      5\u001b[0m vrm_params \u001b[38;5;241m=\u001b[39m vrm_params\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, output_dim)\n\u001b[1;32m----> 7\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmfcc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(pred, vrm_params)\n\u001b[0;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[11], line 48\u001b[0m, in \u001b[0;36mBiLSTM.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     45\u001b[0m attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menergy(torch\u001b[38;5;241m.\u001b[39mcat((hidden, lstm_out), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))))\n\u001b[0;32m     46\u001b[0m context \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbmm(attn, lstm_out)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m---> 48\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlstm_out\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y_pred\n",
      "\u001b[1;31mTypeError\u001b[0m: cat() received an invalid combination of arguments - got (Tensor, Tensor), but expected one of:\n * (tuple of Tensors tensors, int dim, *, Tensor out)\n * (tuple of Tensors tensors, name dim, *, Tensor out)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    start_time = time.time()\n",
    "    for i, (mfcc, vrm_params) in enumerate(train_loader):\n",
    "        mfcc = mfcc.to(device=device).reshape(-1, 1, lstm_input_size)\n",
    "        vrm_params = vrm_params.to(device=device).reshape(-1, 1, output_dim)\n",
    "\n",
    "        pred = model(mfcc)\n",
    "\n",
    "        loss = loss_fn(pred, vrm_params)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    print('Epoch {}/{} \\t loss={:.10f} \\t time={:.2f}s'.format(epoch + 1, n_epochs, loss, time.time() - start_time))\n",
    "\n",
    "    if ((epoch+1) % save_interval == 0):\n",
    "        save_model(model, 'sample-model-' + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%SZ\"), train_file, batch_size, epoch, lr, hidden_state_size, num_sequence_layers, huber_weight, smooth_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_file = 'sample-model-2023-08-12_02-31-16Z'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_file = 'sample-model-' + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%SZ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, weights_file, train_file, batch_size, n_epochs, lr, hidden_state_size, num_sequence_layers, huber_weight, smooth_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTM(lstm_input_size, hidden_state_size, batch_size, output_dim, num_sequence_layers)\n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load('models/{}.pt'.format(weights_file), map_location=device))\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = torch.zeros(output_dim * len(test))\n",
    "test_actual = torch.zeros(output_dim * len(test))\n",
    "\n",
    "for i, (mfcc, vrm_params) in enumerate(test_loader):\n",
    "    mfcc = mfcc.to(device=device).reshape(-1, 1, lstm_input_size)\n",
    "    vrm_params = vrm_params.cpu().reshape(-1)\n",
    "    y_pred = model(mfcc).cpu().detach().reshape(-1)\n",
    "    test_preds[i*batch_size*output_dim:(i+1)*batch_size*output_dim] = y_pred\n",
    "    test_actual[i*batch_size*output_dim:(i+1)*batch_size*output_dim] = vrm_params\n",
    "\n",
    "rmse = torch.sqrt(torch.nn.functional.mse_loss(test_preds, test_actual))\n",
    "print('RMSE: {:.10f}'.format(rmse))\n",
    "    \n",
    "test_preds = test_preds.reshape(len(test), output_dim)\n",
    "test_actual = test_actual.reshape(len(test), output_dim)\n",
    "\n",
    "test_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTION_DATA_PATH = './sampledata/Prediction'\n",
    "\n",
    "def to_csv(test_preds, name='prediction'):\n",
    "    blendshape_params = pd.DataFrame(test_preds[:,:len(BLENDSHAPE_PARAMS)])\n",
    "    blendshape_params.columns = BLENDSHAPE_PARAMS\n",
    "    \n",
    "    bone_params = pd.DataFrame(test_preds[:,len(BLENDSHAPE_PARAMS):])\n",
    "    bone_params.columns = [bone + t for t in ('PosX', 'PosY', 'PosZ', 'RotX', 'RotY', 'RotZ', 'RotW') for bone in BONE_PARAMS]\n",
    "    \n",
    "    STATIC_FRAME = test.STATIC_FRAME\n",
    "    \n",
    "    time_column = pd.DataFrame({'Time': [float(i)*STATIC_FRAME for i in range(len(test_preds))]})\n",
    "    \n",
    "    blendshape_params = pd.concat([time_column, blendshape_params], axis=1)\n",
    "    bone_params = pd.concat([time_column, bone_params], axis=1)\n",
    "    \n",
    "    blendshape_params.to_csv(PREDICTION_DATA_PATH + '/Blendshapes/' + name + '.csv', index=False)\n",
    "    bone_params.to_csv(PREDICTION_DATA_PATH + '/Bones/' + name + '.csv', index=False)\n",
    "    \n",
    "    shutil.copyfile(test.DATA_PATH + '/Audio/' + test.name + '.wav', PREDICTION_DATA_PATH + '/Audio/' + name + '.wav')\n",
    "\n",
    "datestring = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%SZ\")\n",
    "to_csv(test_preds, 'prediction-' + datestring)\n",
    "to_csv(test_actual, 'actual-' + datestring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
