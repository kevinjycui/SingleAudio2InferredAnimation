{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sing2Ani Training Pipeline\n",
    "\n",
    "### Dataset Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soundfile\n",
      "cuda:0\n",
      "2.0.1+cu118\n",
      "2.0.2+cu118\n",
      "----------\n",
      "Source: ./sampledata/Training/Audio/8-8-2023 3-15-42 PM.wav\n",
      "----------\n",
      " - sample_rate: 48000\n",
      " - num_channels: 1\n",
      " - num_frames: 86400000\n",
      " - bits_per_sample: 16\n",
      " - encoding: PCM_S\n",
      " - duration: 1800.0s\n",
      " - num_vrm_frames: 95449\n",
      " - vrm fps: 53.02722222222222\n",
      " - frames_per_vrm: 905.195444687739\n",
      " - seconds_per_vrm: 0.018858238430994562s\n",
      "\n",
      "MFCC with\n",
      " - number of mfcc: 39\n",
      " - number of mels: 78\n",
      " - number of fft: 56\n",
      " - window length: 56\n",
      " - hop length: 28\n",
      " - number of mfcc frames: 3085715\n",
      "\n",
      "----------\n",
      "Source: ./sampledata/Training/Audio/7-17-2023 4-53-18 PM.wav\n",
      "----------\n",
      " - sample_rate: 48000\n",
      " - num_channels: 1\n",
      " - num_frames: 2880000\n",
      " - bits_per_sample: 16\n",
      " - encoding: PCM_S\n",
      " - duration: 60.0s\n",
      " - num_vrm_frames: 537\n",
      " - vrm fps: 8.95\n",
      " - frames_per_vrm: 5363.128491620112\n",
      " - seconds_per_vrm: 0.11173184357541899s\n",
      "\n",
      "MFCC with\n",
      " - number of mfcc: 39\n",
      " - number of mels: 78\n",
      " - number of fft: 335\n",
      " - window length: 335\n",
      " - hop length: 167\n",
      " - number of mfcc frames: 17246\n",
      "\n",
      "----------\n",
      "Source: ./sampledata/Training/Audio/7-17-2023 4-58-09 PM.wav\n",
      "----------\n",
      " - sample_rate: 48000\n",
      " - num_channels: 1\n",
      " - num_frames: 2880000\n",
      " - bits_per_sample: 16\n",
      " - encoding: PCM_S\n",
      " - duration: 60.0s\n",
      " - num_vrm_frames: 539\n",
      " - vrm fps: 8.983333333333333\n",
      " - frames_per_vrm: 5343.228200371058\n",
      " - seconds_per_vrm: 0.11131725417439703s\n",
      "\n",
      "MFCC with\n",
      " - number of mfcc: 39\n",
      " - number of mels: 78\n",
      " - number of fft: 333\n",
      " - window length: 333\n",
      " - hop length: 166\n",
      " - number of mfcc frames: 17350\n",
      "\n",
      "----------\n",
      "Source: ./sampledata/Training/Audio/2023-08-15 10-05-57 PM.wav\n",
      "----------\n",
      " - sample_rate: 48000\n",
      " - num_channels: 1\n",
      " - num_frames: 8640000\n",
      " - bits_per_sample: 16\n",
      " - encoding: PCM_S\n",
      " - duration: 180.0s\n",
      " - num_vrm_frames: 12343\n",
      " - vrm fps: 68.57222222222222\n",
      " - frames_per_vrm: 699.9918982419185\n",
      " - seconds_per_vrm: 0.014583164546706636s\n",
      "\n",
      "MFCC with\n",
      " - number of mfcc: 39\n",
      " - number of mels: 78\n",
      " - number of fft: 43\n",
      " - window length: 43\n",
      " - hop length: 21\n",
      " - number of mfcc frames: 411429\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, 'pytorch-mdn/mdn')\n",
    "import mdn\n",
    "\n",
    "from pydub import AudioSegment\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "import torchaudio.functional as F\n",
    "import torchaudio.transforms as T\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "print(torchaudio.get_audio_backend())\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchaudio.__version__)\n",
    "\n",
    "from IPython.display import Audio\n",
    "from torchaudio.utils import download_asset\n",
    "\n",
    "# torch.random.manual_seed(0)\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()   # interactive mode\n",
    "\n",
    "TRAINING_DATA_PATH = \"./sampledata/Training\" # \"C:/Users/Kevin/AppData/LocalLow/kevinjycui/Training\"\n",
    "TESTING_DATA_PATH = \"./sampledata/Testing\" # \"C:/Users/Kevin/AppData/LocalLow/kevinjycui/Testing\"\n",
    "\n",
    "# VMC Protocol Standard\n",
    "BLENDSHAPE_PARAMS = [\"A\", \"Angry\", \"Blink\", \"Blink_L\", \"Blink_R\", \"E\", \"Fun\", \"I\", \"Joy\", \"LookDown\", \"LookLeft\", \"LookRight\", \"LookUp\", \"Neutral\", \"O\", \"Sorrow\", \"Surprised\", \"U\"]\n",
    "BONE_PARAMS = [\"Chest\", \"Head\", \"Hips\", \"LeftEye\", \"LeftFoot\", \"LeftHand\", \"LeftIndexDistal\", \"LeftIndexIntermediate\", \"LeftIndexProximal\", \"LeftLittleDistal\", \"LeftLittleIntermediate\", \"LeftLittleProximal\", \"LeftLowerArm\", \"LeftLowerLeg\", \"LeftMiddleDistal\", \"LeftMiddleIntermediate\", \"LeftMiddleProximal\", \"LeftRingDistal\", \"LeftRingIntermediate\", \"LeftRingProximal\", \"LeftShoulder\", \"LeftThumbDistal\", \"LeftThumbIntermediate\", \"LeftThumbProximal\", \"LeftToes\", \"LeftUpperArm\", \"LeftUpperLeg\", \"Neck\", \"RightEye\", \"RightFoot\", \"RightHand\", \"RightIndexDistal\", \"RightIndexIntermediate\", \"RightIndexProximal\", \"RightLittleDistal\", \"RightLittleIntermediate\", \"RightLittleProximal\", \"RightLowerArm\", \"RightLowerLeg\", \"RightMiddleDistal\", \"RightMiddleIntermediate\", \"RightMiddleProximal\", \"RightRingDistal\", \"RightRingIntermediate\", \"RightRingProximal\", \"RightShoulder\", \"RightThumbDistal\", \"RightThumbIntermediate\", \"RightThumbProximal\", \"RightToes\", \"RightUpperArm\", \"RightUpperLeg\", \"Spine\", \"UpperChest\"]\n",
    "\n",
    "def print_metadata(metadata, vrm, src=None):\n",
    "    if src:\n",
    "        print(\"-\" * 10)\n",
    "        print(\"Source:\", src)\n",
    "        print(\"-\" * 10)\n",
    "    print(\" - sample_rate:\", metadata.sample_rate)\n",
    "    print(\" - num_channels:\", metadata.num_channels)\n",
    "    print(\" - num_frames:\", metadata.num_frames)\n",
    "    print(\" - bits_per_sample:\", metadata.bits_per_sample)\n",
    "    print(\" - encoding:\", metadata.encoding)\n",
    "    duration = metadata.num_frames / metadata.sample_rate\n",
    "    print(\" - duration:\", duration, end='s\\n')\n",
    "    print(\" - num_vrm_frames:\", len(vrm))\n",
    "    print(\" - vrm fps:\", len(vrm) / duration)\n",
    "    frames_per_vrm = metadata.num_frames / len(vrm)\n",
    "    print(\" - frames_per_vrm:\", frames_per_vrm)\n",
    "    seconds_per_vrm = duration / len(vrm)\n",
    "    print(\" - seconds_per_vrm:\", seconds_per_vrm, end='s\\n')\n",
    "    print()\n",
    "\n",
    "def plot_waveform(waveform, sr, title=\"Waveform\"):\n",
    "    waveform = waveform.numpy()\n",
    "\n",
    "    num_channels, num_frames = waveform.shape\n",
    "    time_axis = torch.arange(0, num_frames) / sr\n",
    "\n",
    "    figure, axes = plt.subplots(num_channels, 1)\n",
    "    if num_channels == 1:\n",
    "        axes.plot(time_axis, waveform[0], linewidth=1)\n",
    "        axes.grid(True)\n",
    "    else:\n",
    "        for axis in axes:\n",
    "            axis.plot(time_axis, waveform[0], linewidth=1)\n",
    "            axis.grid(True)\n",
    "    figure.suptitle(title)\n",
    "\n",
    "def plot_spectrogram(specgram, title=None, ylabel=\"freq_bin\"):\n",
    "    fig, axs = plt.subplots(1, 1)\n",
    "    axs.set_title(title or \"Spectrogram (db)\")\n",
    "    axs.set_ylabel(ylabel)\n",
    "    axs.set_xlabel(\"frame\")\n",
    "    im = axs.imshow(librosa.power_to_db(specgram), origin=\"lower\", aspect=\"auto\")\n",
    "    fig.colorbar(im, ax=axs)\n",
    "\n",
    "class VRMParamsDataset(Dataset):\n",
    "    \"\"\" VRM Parameter Dataset \"\"\"\n",
    "\n",
    "    n_mfcc = 39\n",
    "    n_fft_per_vrm = 16\n",
    "    n_vrmframes = 64\n",
    "\n",
    "    def __init__(self, filename, DATA_PATH=TRAINING_DATA_PATH, effects=None, audio_only=False, scaler=None):\n",
    "        \"\"\"\n",
    "            filename (string): Path to wav/csv files.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.audio_only = audio_only\n",
    "        self.name = Path(filename).stem\n",
    "        self.DATA_PATH = DATA_PATH\n",
    "\n",
    "        self.audio_path = self.DATA_PATH + f\"/Audio/{self.name}.wav\"\n",
    "\n",
    "        self.SPEECH_WAVEFORM, self.SAMPLE_RATE = torchaudio.load(self.audio_path)\n",
    "        self.SPEECH_WAVEFORM = torch.mean(self.SPEECH_WAVEFORM, dim=0).unsqueeze(0)\n",
    "\n",
    "        if self.audio_only:\n",
    "            return\n",
    "\n",
    "        blendshapes = pd.read_csv(DATA_PATH + \"/Blendshapes/\" + self.name + \".csv\")\n",
    "        bones = pd.read_csv(DATA_PATH + \"/Bones/\" + self.name + \".csv\")\n",
    "\n",
    "        # Combine blendshape and bone data\n",
    "        self.data = pd.concat([blendshapes, bones.iloc[:, 1:]], axis=1)\n",
    "\n",
    "        metadata = torchaudio.info(self.audio_path)\n",
    "        print_metadata(metadata, self.data, src=self.audio_path)\n",
    "\n",
    "        # Set static time window for each row\n",
    "        self.STATIC_FRAME = (metadata.num_frames / self.SAMPLE_RATE) / len(self.data)\n",
    "\n",
    "        for idx in range(len(self.data)):\n",
    "            approx_time = (idx + 1) * self.STATIC_FRAME\n",
    "            self.data.iloc[idx, 0] = approx_time\n",
    "\n",
    "        # Normalize data\n",
    "        self.scaler = scaler\n",
    "        if self.scaler == None:\n",
    "            self.scaler = MinMaxScaler(feature_range=(0, 1)).fit(self.data.iloc[:, 1:])\n",
    "\n",
    "        data_scaled = self.scaler.transform(self.data.iloc[:, 1:].values)\n",
    "        data_scaled = pd.DataFrame(data_scaled)\n",
    "        self.data = pd.concat([self.data.iloc[:, 0], data_scaled], axis=1)\n",
    "\n",
    "        # Apply effects such as noise reduction\n",
    "        if effects:\n",
    "            self.SPEECH_WAVEFORM, self.SAMPLE_RATE = torchaudio.sox_effects.apply_effects_tensor(self.SPEECH_WAVEFORM, self.SAMPLE_RATE, effects)\n",
    "\n",
    "        self.init_mfcc()\n",
    "\n",
    "    def init_mfcc(self):\n",
    "        '''\n",
    "        Create MFCC from audio\n",
    "        '''\n",
    "        def mfcc_transform(n_mfcc, n_mels, n_fft, win_length, hop_length):\n",
    "            print(\"MFCC with\")\n",
    "            print(\" - number of mfcc:\", n_mfcc)\n",
    "            print(\" - number of mels:\", n_mels)\n",
    "            print(\" - number of fft:\", n_fft)\n",
    "            print(\" - window length:\", win_length)\n",
    "            print(\" - hop length:\", hop_length)\n",
    "            return T.MFCC(\n",
    "                sample_rate=self.SAMPLE_RATE,\n",
    "                n_mfcc=n_mfcc,\n",
    "                melkwargs={\n",
    "                    \"n_fft\": n_fft,\n",
    "                    \"n_mels\": n_mels,\n",
    "                    \"win_length\": win_length,\n",
    "                    \"hop_length\": hop_length,\n",
    "                    \"window_fn\": torch.hann_window\n",
    "                },\n",
    "            )\n",
    "        # Number of MFCC\n",
    "        n_mfcc = self.n_mfcc\n",
    "        n_mels = n_mfcc * 2\n",
    "        # FFT is every nth of a window, allows for n*2 FFTs per VRM/VMC with potential overlap\n",
    "        n_fft = int(self.STATIC_FRAME * self.SAMPLE_RATE) // self.n_fft_per_vrm\n",
    "        win_length = n_fft\n",
    "        hop_length = n_fft // 2\n",
    "\n",
    "        self.mfcc = mfcc_transform(n_mfcc, n_mels, n_fft, win_length, hop_length)(self.SPEECH_WAVEFORM)\n",
    "        self.hop_length = hop_length\n",
    "        print(\" - number of mfcc frames:\", len(self.mfcc[0][0]))\n",
    "        print()\n",
    "\n",
    "    def input_dim(self):\n",
    "        return self.n_mfcc * self.n_vrmframes\n",
    "    \n",
    "    def output_dim(self):\n",
    "        return len(self.data.iloc[0]) - 1\n",
    "\n",
    "    def _set_static_frame(self, _sf):\n",
    "        self.STATIC_FRAME = _sf\n",
    "        self.init_mfcc()\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.audio_only:\n",
    "            metadata = torchaudio.info(self.audio_path)\n",
    "            return int((metadata.num_frames / metadata.sample_rate) / self.STATIC_FRAME)\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        # Number of frames before and after timestamp to get\n",
    "        n_vrmframes_half = self.n_vrmframes // 2\n",
    "\n",
    "        mfcc_index = min((idx + 1) * n_vrmframes_half, self.mfcc.shape[2] - n_vrmframes_half - 1)\n",
    "        mfcc_frame = torch.zeros(self.n_mfcc, self.n_vrmframes)\n",
    "        # print(idx, mfcc_index, n_vrmframes_half)\n",
    "        mfcc_frame[:] = self.mfcc[0,:,mfcc_index - n_vrmframes_half:mfcc_index + n_vrmframes_half]\n",
    "\n",
    "        if self.audio_only:\n",
    "            return mfcc_frame, torch.empty(1)\n",
    "                \n",
    "        time_window = self.data.iloc[idx, 0]\n",
    "\n",
    "        vrm_params = self.data.iloc[idx, 1:]\n",
    "        vrm_params = np.asarray(vrm_params)\n",
    "        vrm_params = vrm_params.astype('float')\n",
    "\n",
    "        return mfcc_frame, torch.Tensor(vrm_params)\n",
    "    \n",
    "\n",
    "effect = [[\"sinc\", \"300-3k\"]]\n",
    "effect = None # libsox not available for Windows\n",
    "\n",
    "train_file = [\"8-8-2023 3-15-42 PM\"]\n",
    "valid_file = \"2023-08-15 10-05-57 PM\"\n",
    "\n",
    "train0 = VRMParamsDataset(train_file[0], TRAINING_DATA_PATH, effect)\n",
    "train = [VRMParamsDataset(file, TRAINING_DATA_PATH, effect, scaler=train0.scaler) for file in train_file[1:]]\n",
    "train.insert(0, train0)\n",
    "assert all([t.input_dim() == train0.input_dim() and t.output_dim() == train0.output_dim() for t in train])\n",
    "\n",
    "valid = VRMParamsDataset(valid_file, TRAINING_DATA_PATH, effect, scaler=train0.scaler)\n",
    "test = VRMParamsDataset(\"7-17-2023 4-53-18 PM\", TRAINING_DATA_PATH, effect, scaler=train0.scaler)\n",
    "# test._set_static_frame(train.STATIC_FRAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot waveforms and spectrograms\n",
    "VISUAL = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play audio of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (VISUAL):\n",
    "    Audio(train[0].SPEECH_WAVEFORM, rate=train[0].SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play audio of testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (VISUAL):\n",
    "    Audio(test.SPEECH_WAVEFORM, rate=test.SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Waveform of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (VISUAL):\n",
    "    plot_waveform(train[0].SPEECH_WAVEFORM, train.SAMPLE_RATE, title=\"Training audio\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([39, 3085715])\n"
     ]
    }
   ],
   "source": [
    "print(train[0].mfcc[0].shape)\n",
    "if (VISUAL):\n",
    "    plot_spectrogram(train[0].mfcc[0])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Waveform of testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (VISUAL):\n",
    "    plot_waveform(test.SPEECH_WAVEFORM, test.SAMPLE_RATE, title=\"Testing audio\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([39, 411429])\n"
     ]
    }
   ],
   "source": [
    "print(test.mfcc[0].shape)\n",
    "if (VISUAL):\n",
    "    plot_spectrogram(test.mfcc[0])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([39, 64]), torch.Size([396]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 100\n",
    "\n",
    "train_loader = [DataLoader(t, batch_size=batch_size, shuffle=False) for t in train]\n",
    "valid_loader = DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "_x, _y = train[0][0]\n",
    "_x.shape, _y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VRMLoss(nn.Module):\n",
    "    bs_smooth_diff = 0.1\n",
    "    bone_smooth_diff = 0.4\n",
    "\n",
    "    def __init__(self, weights): #, non_expr_face_bs_weight=1.0):\n",
    "        super(VRMLoss, self).__init__()\n",
    "        self.bs_huber_weight = weights['bs']['huber']\n",
    "        self.bs_smooth_weight = weights['bs']['smooth']\n",
    "\n",
    "        self.bone_huber_weight = weights['bone']['huber']\n",
    "        self.bone_smooth_weight = weights['bone']['smooth']\n",
    "        \n",
    "        # self.non_expr_face_bs_weight = non_expr_face_bs_weight\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        # Separate data into blendshape and bone\n",
    "        bs_output = output.squeeze()[:len(BLENDSHAPE_PARAMS)]\n",
    "        bs_target = target.squeeze()[:len(BLENDSHAPE_PARAMS)]\n",
    "\n",
    "        bone_output = output.squeeze()[len(BLENDSHAPE_PARAMS):]\n",
    "        bone_target = target.squeeze()[len(BLENDSHAPE_PARAMS):]\n",
    "        \n",
    "        # n_loss = torch.nn.MSELoss()(bs_output, bs_target)\n",
    "\n",
    "        # def expr_blendshapes(blendshapes):\n",
    "        #     params = ['A', 'Angry', 'E', 'Fun', 'I', 'Joy', 'Neutral', 'O', 'U', 'Sorrow', 'Surprised']\n",
    "        #     # print(blendshapes.shape)\n",
    "        #     return torch.cat(tuple([blendshapes[:,BLENDSHAPE_PARAMS.index(p)] for p in params]), dim=0)\n",
    "\n",
    "        # bs_output = expr_blendshapes(bs_output)\n",
    "        # bs_target = expr_blendshapes(bs_target)\n",
    "        \n",
    "        bs_prev = None\n",
    "        bs_smooth = 0.\n",
    "        \n",
    "        bone_prev = None\n",
    "        bone_smooth = 0.\n",
    "\n",
    "        bs_huber = nn.HuberLoss()(bs_output, bs_target)\n",
    "        for sample in range(len(bs_target)):\n",
    "            # print(output[sample].shape, target[sample].shape)\n",
    "            # print(bs_output[sample].shape, bs_target[sample].shape)\n",
    "            bs_smooth += 0 if bs_prev is None else 1. - abs(self.bs_smooth_diff - nn.CosineSimilarity(dim=0)(bs_output[sample], bs_prev).mean())\n",
    "            bs_prev = bs_output[sample]\n",
    "        \n",
    "        bone_huber = nn.HuberLoss()(bone_output, bone_target)\n",
    "        for sample in range(len(bone_target)):\n",
    "            bone_smooth += 0 if bone_prev is None else 1. - abs(self.bone_smooth_diff - nn.CosineSimilarity(dim=0)(bone_output[sample], bone_prev).mean())\n",
    "            bone_prev = bone_output[sample]\n",
    "\n",
    "        def a2f_loss(huber, smooth, length, w=[1.0, 1.0]):\n",
    "            # Calculate loss as in Audio2Face (Guanzhong Tian; Yi Yuan; Yong Liu)\n",
    "            return (w[0]*huber + w[1]*smooth) / length\n",
    "\n",
    "        # Loss is computed separately for blendshape and bone with different weights and combined\n",
    "        return a2f_loss(bs_huber, bs_smooth, len(bs_target), [self.bs_huber_weight, self.bs_smooth_weight]) + a2f_loss(bone_huber, bone_smooth, len(bone_target), [self.bone_huber_weight, self.bone_smooth_weight]) # + self.non_expr_face_bs_weight * n_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, batch_size, output_dim, num_layers, p):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            input_dim: Input layer dimension\n",
    "            hidden_dim: Hidden layer dimension\n",
    "            batch_size: Batch size of data\n",
    "            output_dim: Output layer dimension\n",
    "            num_layers: Number of layers\n",
    "            p: Dropout\n",
    "        \"\"\"\n",
    "        super(BiLSTM, self).__init__()\n",
    "\n",
    "        # Dropout to prevent overfitting\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # self.init_linear = nn.Linear(self.input_dim, self.input_dim)\n",
    "\n",
    "        # Bidirectional LSTM to predict sequence with memory\n",
    "        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, self.num_layers, batch_first=True, bidirectional=True, dropout=p)\n",
    "\n",
    "        # Mixed Density Network layer to prevent sequence from staying still, outputs a probability density function\n",
    "        self.mdn = mdn.MDN(self.hidden_dim * 2, self.hidden_dim * 2, 1)\n",
    "\n",
    "        # Attention layer to determine important parameters from hidden weights\n",
    "        self.linear_hidden = nn.Linear(self.hidden_dim * 2, self.hidden_dim)\n",
    "        self.energy = nn.Linear(self.hidden_dim*3, 1)\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Fully connected layer to produce output\n",
    "        self.fc = nn.Linear(self.hidden_dim*4, output_dim)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # Hidden layers of LSTM initialised as Gaussian distribution\n",
    "        w1 = torch.randn(self.num_layers, self.batch_size, self.hidden_dim)\n",
    "        w2 = torch.randn(self.num_layers, self.batch_size, self.hidden_dim)\n",
    "        return w1, w2\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = self.dropout(input)\n",
    "\n",
    "        # print('----------------------------------------------------------------------------------')\n",
    "        # print(input)\n",
    "        \n",
    "        lstm_out, (hidden, cell) = self.lstm(input)\n",
    "        # print('               LSTM')\n",
    "        # print(lstm_out)\n",
    "\n",
    "        # print('               MDN')\n",
    "        # print(mdn_out)\n",
    "\n",
    "        # print(hidden[0:2].shape)\n",
    "        \n",
    "        hidden = self.linear_hidden(hidden[0:2].reshape(1, -1, self.hidden_dim * 2)).permute(1, 0, 2)\n",
    "        # print('               HIDD')\n",
    "        # print(hidden)\n",
    "\n",
    "        attn = self.softmax(self.relu(self.energy(torch.cat((hidden, lstm_out), dim=2))))\n",
    "        # print('               ATTN')\n",
    "        # print(attn)\n",
    "        context = torch.bmm(attn, lstm_out).permute(1, 0, 2)\n",
    "        mdn_out = self.mdn(context)[2].permute(1, 0, 2)\n",
    "\n",
    "        # print(context.shape, lstm_out.shape)\n",
    "        y_pred = self.fc(torch.cat((mdn_out, lstm_out.permute(1, 0, 2)), dim=2)).squeeze()\n",
    "        # print('               PRED')\n",
    "        # print(y_pred)\n",
    "        return y_pred\n",
    "\n",
    "# Training hyperparameters\n",
    "n_epochs = 30\n",
    "lr = 0.0001\n",
    "lstm_input_size = train[0].input_dim()\n",
    "hidden_state_size = 512\n",
    "num_sequence_layers = 2\n",
    "output_dim = train[0].output_dim()\n",
    "save_interval = 10\n",
    "dropoff = 0.0\n",
    "\n",
    "model = BiLSTM(lstm_input_size, hidden_state_size, batch_size, output_dim, num_sequence_layers, dropoff)\n",
    "model = model.to(device)\n",
    "\n",
    "# Adam optimizer (gradient descent)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'bs': {\n",
    "        'huber': 9.0,\n",
    "        'smooth': 0.2\n",
    "    },\n",
    "    'bone': {\n",
    "        'huber': 1.0,\n",
    "        'smooth': 0.2\n",
    "    }\n",
    "}\n",
    "loss_fn = VRMLoss(weights)\n",
    "valid_loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_state(state_dict, name, train_file, batch_size, epochs, lr, hidden_state_size, num_sequence_layers, weights, valid_loss):\n",
    "    torch.save(state_dict, 'models/{}.pt'.format(name))\n",
    "    with open('models/meta-{}.txt'.format(name), 'w') as f:\n",
    "        metastring = 'Train file: {}\\nBatch size: {}\\nNumber of epochs: {}\\nLearning rate: {}\\nHidden state size: {}\\nNumber of LSTM layers: {}\\nWeights: {}\\nValidation loss: {}'\n",
    "        f.write(metastring.format(train_file, batch_size, epochs, lr, hidden_state_size, num_sequence_layers, str(weights), valid_loss))\n",
    "\n",
    "def save_model(model, name, train_file, batch_size, epochs, lr, hidden_state_size, num_sequence_layers, weights, valid_loss):\n",
    "    save_model_state(model.state_dict(), name, train_file, batch_size, epochs, lr, hidden_state_size, num_sequence_layers, weights, valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 \tTrain loss=0.1001580049 \tValid loss=0.0080250793\tTime=92.62s\n",
      "Best model: Model at epoch 1 with valid loss 0.0080250793\n",
      "Epoch 2/30 \tTrain loss=0.0995194562 \tValid loss=0.0073329620\tTime=92.40s\n",
      "Best model: Model at epoch 2 with valid loss 0.0073329620\n",
      "Epoch 3/30 \tTrain loss=0.0995294764 \tValid loss=0.0073845716\tTime=92.37s\n",
      "Epoch 4/30 \tTrain loss=0.0995377378 \tValid loss=0.0073998995\tTime=95.35s\n",
      "Epoch 5/30 \tTrain loss=0.0995425839 \tValid loss=0.0074140476\tTime=94.46s\n",
      "Epoch 6/30 \tTrain loss=0.0995465403 \tValid loss=0.0074324143\tTime=94.36s\n",
      "Epoch 7/30 \tTrain loss=0.0995503215 \tValid loss=0.0074371019\tTime=94.36s\n",
      "Epoch 8/30 \tTrain loss=0.0995535396 \tValid loss=0.0074490316\tTime=94.49s\n",
      "Epoch 9/30 \tTrain loss=0.0995561626 \tValid loss=0.0074461555\tTime=94.04s\n",
      "Epoch 10/30 \tTrain loss=0.0995585641 \tValid loss=0.0074552527\tTime=94.35s\n",
      "Epoch 11/30 \tTrain loss=0.0995606384 \tValid loss=0.0074477620\tTime=94.39s\n",
      "Epoch 12/30 \tTrain loss=0.0995620016 \tValid loss=0.0074486457\tTime=95.35s\n",
      "Epoch 13/30 \tTrain loss=0.0995630225 \tValid loss=0.0074350146\tTime=95.96s\n",
      "Epoch 14/30 \tTrain loss=0.0995636771 \tValid loss=0.0074362980\tTime=94.63s\n",
      "Epoch 15/30 \tTrain loss=0.0995641896 \tValid loss=0.0074301296\tTime=94.62s\n",
      "Epoch 16/30 \tTrain loss=0.0995645528 \tValid loss=0.0074241614\tTime=95.45s\n",
      "Epoch 17/30 \tTrain loss=0.0995649263 \tValid loss=0.0074179145\tTime=94.65s\n",
      "Epoch 18/30 \tTrain loss=0.0995652985 \tValid loss=0.0074098506\tTime=94.46s\n",
      "Epoch 19/30 \tTrain loss=0.0995656626 \tValid loss=0.0074030115\tTime=94.55s\n",
      "Epoch 20/30 \tTrain loss=0.0995659924 \tValid loss=0.0073977145\tTime=95.40s\n",
      "Epoch 21/30 \tTrain loss=0.0995662545 \tValid loss=0.0073864383\tTime=94.53s\n",
      "Epoch 22/30 \tTrain loss=0.0995662048 \tValid loss=0.0073749255\tTime=94.53s\n",
      "Epoch 23/30 \tTrain loss=0.0995662918 \tValid loss=0.0073657067\tTime=94.62s\n",
      "Epoch 24/30 \tTrain loss=0.0995663095 \tValid loss=0.0073606702\tTime=94.40s\n",
      "Epoch 25/30 \tTrain loss=0.0995663442 \tValid loss=0.0073484096\tTime=95.57s\n",
      "Epoch 26/30 \tTrain loss=0.0995661184 \tValid loss=0.0073427330\tTime=95.13s\n",
      "Epoch 27/30 \tTrain loss=0.0995658822 \tValid loss=0.0073365750\tTime=94.86s\n",
      "Epoch 28/30 \tTrain loss=0.0995657617 \tValid loss=0.0073296613\tTime=94.42s\n",
      "Best model: Model at epoch 28 with valid loss 0.0073296613\n",
      "Epoch 29/30 \tTrain loss=0.0995662431 \tValid loss=0.0073193250\tTime=95.14s\n",
      "Best model: Model at epoch 29 with valid loss 0.0073193250\n",
      "Epoch 30/30 \tTrain loss=0.0995673678 \tValid loss=0.0073000114\tTime=94.49s\n",
      "Best model: Model at epoch 30 with valid loss 0.0073000114\n",
      "--\n",
      "Best model: Model at epoch 30 with valid loss 0.0073000114\n"
     ]
    }
   ],
   "source": [
    "min_valid_loss = np.inf\n",
    "best_model = None\n",
    "best_model_epoch = -2\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        train_loss = 0.0\n",
    "        model.train()\n",
    "        for loader in train_loader:\n",
    "            for i, (mfcc, vrm_params) in enumerate(loader):\n",
    "                mfcc = mfcc.to(device=device).reshape(-1, 1, lstm_input_size)\n",
    "                vrm_params = vrm_params.to(device=device).reshape(-1, 1, output_dim)\n",
    "        \n",
    "                pred = model(mfcc)\n",
    "        \n",
    "                loss = loss_fn(pred, vrm_params)\n",
    "        \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "            \n",
    "        train_loss /= sum([len(loader) for loader in train_loader])\n",
    "            \n",
    "        valid_loss = 0.0\n",
    "        model.eval()\n",
    "        for i, (mfcc, vrm_params) in enumerate(valid_loader):\n",
    "            mfcc = mfcc.to(device=device).reshape(-1, 1, lstm_input_size)\n",
    "            vrm_params = vrm_params.to(device=device).reshape(-1, 1, output_dim)\n",
    "            \n",
    "            pred = model(mfcc)\n",
    "            \n",
    "            loss = valid_loss_fn(pred, vrm_params)\n",
    "            valid_loss += loss.item()\n",
    "    \n",
    "        valid_loss /= len(valid_loader)\n",
    "    \n",
    "        print('Epoch {}/{} \\tTrain loss={:.10f} \\tValid loss={:.10f}\\tTime={:.2f}s'.format(epoch + 1, n_epochs, train_loss, valid_loss, time.time() - start_time))\n",
    "    \n",
    "        if ((epoch+1) % save_interval == 0):\n",
    "            save_model(model, 'sample-model_' + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%SZ\") + '_EPOCH' + str(epoch+1), train_file, batch_size, epoch+1, lr, hidden_state_size, num_sequence_layers, weights, valid_loss)\n",
    "        \n",
    "        if min_valid_loss > valid_loss:\n",
    "            min_valid_loss = valid_loss\n",
    "            best_model = model.state_dict()\n",
    "            best_model_epoch = epoch\n",
    "    \n",
    "            print('Best model: Model at epoch {} with valid loss {:.10f}'.format(best_model_epoch+1, min_valid_loss))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "print('--\\nBest model: Model at epoch {} with valid loss {:.10f}'.format(best_model_epoch+1, min_valid_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_file = 'sample-model_2023-08-16_22-22-02Z_EPOCH60'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_file = 'model_' + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%SZ\")\n",
    "# Save latest model\n",
    "save_model(model, weights_file + '_LATEST', str(train_file), batch_size, n_epochs, lr, hidden_state_size, num_sequence_layers, weights, valid_loss)\n",
    "# Save best model\n",
    "save_model_state(best_model, weights_file + '_BEST', str(train_file), batch_size, best_model_epoch+1, lr, hidden_state_size, num_sequence_layers, weights, min_valid_loss)\n",
    "weights_file += '_BEST'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTM(\n",
       "  (dropout): Dropout(p=0.0, inplace=False)\n",
       "  (lstm): LSTM(2496, 512, num_layers=2, batch_first=True, bidirectional=True)\n",
       "  (mdn): MDN(\n",
       "    (pi): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=1, bias=True)\n",
       "      (1): Softmax(dim=1)\n",
       "    )\n",
       "    (sigma): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (mu): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  )\n",
       "  (linear_hidden): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (energy): Linear(in_features=1536, out_features=1, bias=True)\n",
       "  (softmax): Softmax(dim=0)\n",
       "  (relu): ReLU()\n",
       "  (fc): Linear(in_features=2048, out_features=396, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BiLSTM(lstm_input_size, hidden_state_size, batch_size, output_dim, num_sequence_layers, dropoff)\n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load('models/{}.pt'.format(weights_file), map_location=device))\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTM(\n",
       "  (dropout): Dropout(p=0.0, inplace=False)\n",
       "  (lstm): LSTM(2496, 512, num_layers=2, batch_first=True, bidirectional=True)\n",
       "  (mdn): MDN(\n",
       "    (pi): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=1, bias=True)\n",
       "      (1): Softmax(dim=1)\n",
       "    )\n",
       "    (sigma): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (mu): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  )\n",
       "  (linear_hidden): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (energy): Linear(in_features=1536, out_features=1, bias=True)\n",
       "  (softmax): Softmax(dim=0)\n",
       "  (relu): ReLU()\n",
       "  (fc): Linear(in_features=2048, out_features=396, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.1353097111\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([12343, 396])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds = torch.zeros(output_dim * len(test))\n",
    "test_actual = torch.zeros(output_dim * len(test))\n",
    "\n",
    "for i, (mfcc, vrm_params) in enumerate(test_loader):\n",
    "    mfcc = mfcc.to(device=device).reshape(-1, 1, lstm_input_size)\n",
    "    if not test.audio_only:\n",
    "        vrm_params = vrm_params.cpu().reshape(-1)\n",
    "    y_pred = model(mfcc).cpu().detach().reshape(-1)\n",
    "    test_preds[i*batch_size*output_dim:(i+1)*batch_size*output_dim] = y_pred\n",
    "    if not test.audio_only:\n",
    "        test_actual[i*batch_size*output_dim:(i+1)*batch_size*output_dim] = vrm_params\n",
    "\n",
    "if not test.audio_only:\n",
    "    rmse = torch.sqrt(torch.nn.functional.mse_loss(test_preds, test_actual))\n",
    "    print('RMSE: {:.10f}'.format(rmse))\n",
    "        \n",
    "test_preds = test_preds.reshape(len(test), output_dim)\n",
    "test_actual = test_actual.reshape(len(test), output_dim)\n",
    "\n",
    "test_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1788,  0.0004,  0.1114,  ...,  0.0060,  0.0003, -0.0004],\n",
       "        [ 0.1788,  0.0004,  0.1114,  ...,  0.0060,  0.0003, -0.0004],\n",
       "        [ 0.1788,  0.0004,  0.1114,  ...,  0.0060,  0.0003, -0.0004],\n",
       "        ...,\n",
       "        [ 0.1660, -0.0003,  0.1143,  ...,  0.0054,  0.0008,  0.0007],\n",
       "        [ 0.1660, -0.0003,  0.1143,  ...,  0.0054,  0.0008,  0.0007],\n",
       "        [ 0.1660, -0.0003,  0.1143,  ...,  0.0054,  0.0008,  0.0007]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTION_DATA_PATH = './sampledata/Prediction'\n",
    "\n",
    "def to_csv(test_preds, name='prediction'):\n",
    "    test_preds = train[0].scaler.inverse_transform(test_preds)\n",
    "    \n",
    "    blendshape_params = pd.DataFrame(test_preds[:,:len(BLENDSHAPE_PARAMS)])\n",
    "    blendshape_params.columns = BLENDSHAPE_PARAMS\n",
    "    \n",
    "    bone_params = pd.DataFrame(test_preds[:,len(BLENDSHAPE_PARAMS):])\n",
    "    bone_params.columns = [bone + t for t in ('PosX', 'PosY', 'PosZ', 'RotX', 'RotY', 'RotZ', 'RotW') for bone in BONE_PARAMS]\n",
    "    \n",
    "    STATIC_FRAME = test.STATIC_FRAME\n",
    "    \n",
    "    time_column = pd.DataFrame({'Time': [float(i)*STATIC_FRAME for i in range(len(test_preds))]})\n",
    "    \n",
    "    blendshape_params = pd.concat([time_column, blendshape_params], axis=1)\n",
    "    bone_params = pd.concat([time_column, bone_params], axis=1)\n",
    "    \n",
    "    blendshape_params.to_csv(PREDICTION_DATA_PATH + '/Blendshapes/' + name + '.csv', index=False)\n",
    "    bone_params.to_csv(PREDICTION_DATA_PATH + '/Bones/' + name + '.csv', index=False)\n",
    "    \n",
    "    shutil.copyfile(test.DATA_PATH + '/Audio/' + test.name + '.wav', PREDICTION_DATA_PATH + '/Audio/' + name + '.wav')\n",
    "\n",
    "datestring = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%SZ\")\n",
    "to_csv(test_preds, 'prediction-' + datestring)\n",
    "if not test.audio_only:\n",
    "    to_csv(test_actual, 'actual-' + datestring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
