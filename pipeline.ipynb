{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sing2Ani Training Pipeline\n",
    "\n",
    "### Dataset Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soundfile\n",
      "cuda:0\n",
      "2.0.1+cu118\n",
      "2.0.2+cu118\n",
      "----------\n",
      "Source: ./sampledata/Training/Audio/8-8-2023 3-15-42 PM.wav\n",
      "----------\n",
      " - sample_rate: 48000\n",
      " - num_channels: 1\n",
      " - num_frames: 86400000\n",
      " - bits_per_sample: 16\n",
      " - encoding: PCM_S\n",
      " - duration: 1800.0s\n",
      " - num_blendshape_frames: 95449\n",
      " - num_bone_frames: 95449\n",
      " - blendshape fps: 53.02722222222222\n",
      " - bone fps: 53.02722222222222\n",
      " - frames per blendshape: 905.195444687739\n",
      " - frames per bone: 905.195444687739\n",
      " - seconds per blendshape: 0.018858238430994562s\n",
      " - seconds per bone: 0.018858238430994562s\n",
      "\n",
      "MFCC with\n",
      " - number of mfcc: 39\n",
      " - number of mels: 78\n",
      " - number of fft: 56\n",
      " - window length: 56\n",
      " - hop length: 28\n",
      " - number of mfcc frames: 3085715\n",
      "\n",
      "----------\n",
      "Source: ./sampledata/Training/Audio/2023-08-15 10-05-57 PM.wav\n",
      "----------\n",
      " - sample_rate: 48000\n",
      " - num_channels: 1\n",
      " - num_frames: 8640000\n",
      " - bits_per_sample: 16\n",
      " - encoding: PCM_S\n",
      " - duration: 180.0s\n",
      " - num_blendshape_frames: 12343\n",
      " - num_bone_frames: 12343\n",
      " - blendshape fps: 68.57222222222222\n",
      " - bone fps: 68.57222222222222\n",
      " - frames per blendshape: 699.9918982419185\n",
      " - frames per bone: 699.9918982419185\n",
      " - seconds per blendshape: 0.014583164546706636s\n",
      " - seconds per bone: 0.014583164546706636s\n",
      "\n",
      "MFCC with\n",
      " - number of mfcc: 39\n",
      " - number of mels: 78\n",
      " - number of fft: 43\n",
      " - window length: 43\n",
      " - hop length: 21\n",
      " - number of mfcc frames: 411429\n",
      "\n",
      "----------\n",
      "Source: ./sampledata/Training/Audio/7-17-2023 4-53-18 PM.wav\n",
      "----------\n",
      " - sample_rate: 48000\n",
      " - num_channels: 1\n",
      " - num_frames: 2880000\n",
      " - bits_per_sample: 16\n",
      " - encoding: PCM_S\n",
      " - duration: 60.0s\n",
      " - num_blendshape_frames: 537\n",
      " - num_bone_frames: 537\n",
      " - blendshape fps: 8.95\n",
      " - bone fps: 8.95\n",
      " - frames per blendshape: 5363.128491620112\n",
      " - frames per bone: 5363.128491620112\n",
      " - seconds per blendshape: 0.11173184357541899s\n",
      " - seconds per bone: 0.11173184357541899s\n",
      "\n",
      "MFCC with\n",
      " - number of mfcc: 39\n",
      " - number of mels: 78\n",
      " - number of fft: 335\n",
      " - window length: 335\n",
      " - hop length: 167\n",
      " - number of mfcc frames: 17246\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, 'pytorch-mdn/mdn')\n",
    "import mdn\n",
    "\n",
    "from pydub import AudioSegment\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "import torchaudio.functional as F\n",
    "import torchaudio.transforms as T\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "print(torchaudio.get_audio_backend())\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchaudio.__version__)\n",
    "\n",
    "from IPython.display import Audio\n",
    "from torchaudio.utils import download_asset\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()   # interactive mode\n",
    "\n",
    "TRAINING_DATA_PATH = \"./sampledata/Training\" # \"C:/Users/Kevin/AppData/LocalLow/kevinjycui/Training\"\n",
    "TESTING_DATA_PATH = \"./sampledata/Testing\" # \"C:/Users/Kevin/AppData/LocalLow/kevinjycui/Testing\"\n",
    "\n",
    "BLENDSHAPE_PARAMS = [\"A\", \"Angry\", \"Blink\", \"Blink_L\", \"Blink_R\", \"E\", \"Fun\", \"I\", \"Joy\", \"LookDown\", \"LookLeft\", \"LookRight\", \"LookUp\", \"Neutral\", \"O\", \"Sorrow\", \"Surprised\", \"U\"]\n",
    "BONE_PARAMS = [\"Chest\", \"Head\", \"Hips\", \"LeftEye\", \"LeftFoot\", \"LeftHand\", \"LeftIndexDistal\", \"LeftIndexIntermediate\", \"LeftIndexProximal\", \"LeftLittleDistal\", \"LeftLittleIntermediate\", \"LeftLittleProximal\", \"LeftLowerArm\", \"LeftLowerLeg\", \"LeftMiddleDistal\", \"LeftMiddleIntermediate\", \"LeftMiddleProximal\", \"LeftRingDistal\", \"LeftRingIntermediate\", \"LeftRingProximal\", \"LeftShoulder\", \"LeftThumbDistal\", \"LeftThumbIntermediate\", \"LeftThumbProximal\", \"LeftToes\", \"LeftUpperArm\", \"LeftUpperLeg\", \"Neck\", \"RightEye\", \"RightFoot\", \"RightHand\", \"RightIndexDistal\", \"RightIndexIntermediate\", \"RightIndexProximal\", \"RightLittleDistal\", \"RightLittleIntermediate\", \"RightLittleProximal\", \"RightLowerArm\", \"RightLowerLeg\", \"RightMiddleDistal\", \"RightMiddleIntermediate\", \"RightMiddleProximal\", \"RightRingDistal\", \"RightRingIntermediate\", \"RightRingProximal\", \"RightShoulder\", \"RightThumbDistal\", \"RightThumbIntermediate\", \"RightThumbProximal\", \"RightToes\", \"RightUpperArm\", \"RightUpperLeg\", \"Spine\", \"UpperChest\"]\n",
    "\n",
    "def print_metadata(metadata, blendshapes, bones, src=None):\n",
    "    if src:\n",
    "        print(\"-\" * 10)\n",
    "        print(\"Source:\", src)\n",
    "        print(\"-\" * 10)\n",
    "    print(\" - sample_rate:\", metadata.sample_rate)\n",
    "    print(\" - num_channels:\", metadata.num_channels)\n",
    "    print(\" - num_frames:\", metadata.num_frames)\n",
    "    print(\" - bits_per_sample:\", metadata.bits_per_sample)\n",
    "    print(\" - encoding:\", metadata.encoding)\n",
    "    duration = metadata.num_frames / metadata.sample_rate\n",
    "    print(\" - duration:\", duration, end='s\\n')\n",
    "    print(\" - num_blendshape_frames:\", len(blendshapes))\n",
    "    print(\" - num_bone_frames:\", len(bones))\n",
    "    print(\" - blendshape fps:\", len(blendshapes) / duration)\n",
    "    print(\" - bone fps:\", len(bones) / duration)\n",
    "    frames_per_blendshape = metadata.num_frames / len(blendshapes)\n",
    "    frames_per_bone = metadata.num_frames / len(bones)\n",
    "    print(\" - frames per blendshape:\", frames_per_blendshape)\n",
    "    print(\" - frames per bone:\", frames_per_bone)\n",
    "    seconds_per_blendshape = duration / len(blendshapes)\n",
    "    seconds_per_bone = duration / len(bones)\n",
    "    print(\" - seconds per blendshape:\", seconds_per_blendshape, end='s\\n')\n",
    "    print(\" - seconds per bone:\", seconds_per_bone, end='s\\n')\n",
    "    assert frames_per_blendshape == frames_per_bone and seconds_per_blendshape == seconds_per_bone\n",
    "    print()\n",
    "\n",
    "def plot_waveform(waveform, sr, title=\"Waveform\"):\n",
    "    waveform = waveform.numpy()\n",
    "\n",
    "    num_channels, num_frames = waveform.shape\n",
    "    time_axis = torch.arange(0, num_frames) / sr\n",
    "\n",
    "    figure, axes = plt.subplots(num_channels, 1)\n",
    "    if num_channels == 1:\n",
    "        axes.plot(time_axis, waveform[0], linewidth=1)\n",
    "        axes.grid(True)\n",
    "    else:\n",
    "        for axis in axes:\n",
    "            axis.plot(time_axis, waveform[0], linewidth=1)\n",
    "            axis.grid(True)\n",
    "    figure.suptitle(title)\n",
    "\n",
    "def plot_spectrogram(specgram, title=None, ylabel=\"freq_bin\"):\n",
    "    fig, axs = plt.subplots(1, 1)\n",
    "    axs.set_title(title or \"Spectrogram (db)\")\n",
    "    axs.set_ylabel(ylabel)\n",
    "    axs.set_xlabel(\"frame\")\n",
    "    im = axs.imshow(librosa.power_to_db(specgram), origin=\"lower\", aspect=\"auto\")\n",
    "    fig.colorbar(im, ax=axs)\n",
    "\n",
    "class VRMParamsDataset(Dataset):\n",
    "    \"\"\" VRM Parameter Dataset \"\"\"\n",
    "\n",
    "    n_mfcc = 39\n",
    "    n_vrmframes = 64\n",
    "\n",
    "    def __init__(self, filename, DATA_PATH=TRAINING_DATA_PATH, effects=None, audio_only=False, bs_scaler=None, bone_scaler=None):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            filename (string): Path to wav/csv files.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.audio_only = audio_only\n",
    "        self.name = Path(filename).stem\n",
    "        self.DATA_PATH = DATA_PATH\n",
    "\n",
    "        self.audio_path = self.DATA_PATH + f\"/Audio/{self.name}.wav\"\n",
    "\n",
    "        self.SPEECH_WAVEFORM, self.SAMPLE_RATE = torchaudio.load(self.audio_path)\n",
    "        self.SPEECH_WAVEFORM = torch.mean(self.SPEECH_WAVEFORM, dim=0).unsqueeze(0)\n",
    "\n",
    "        if self.audio_only:\n",
    "            return\n",
    "\n",
    "        self.blendshapes = pd.read_csv(DATA_PATH + \"/Blendshapes/\" + self.name + \".csv\")\n",
    "        self.bones = pd.read_csv(DATA_PATH + \"/Bones/\" + self.name + \".csv\")\n",
    "\n",
    "        self.bs_scaler = bs_scaler\n",
    "        if self.bs_scaler == None:\n",
    "            self.bs_scaler = MinMaxScaler(feature_range=(0, 1)).fit(self.blendshapes.values)\n",
    "        bs_scaled = self.bs_scaler.transform(self.blendshapes.values)\n",
    "        self.blendshape = pd.DataFrame(bs_scaled)\n",
    "\n",
    "        self.bone_scaler = bone_scaler\n",
    "        if self.bone_scaler == None:\n",
    "            self.bone_scaler = MinMaxScaler(feature_range=(0, 1)).fit(self.bones.values)\n",
    "        bone_scaled = self.bone_scaler.transform(self.bones.values)\n",
    "        self.bones = pd.DataFrame(bone_scaled)\n",
    "\n",
    "        assert len(self.blendshapes) == len(self.bones)\n",
    "\n",
    "        metadata = torchaudio.info(self.audio_path)\n",
    "        print_metadata(metadata, self.blendshapes, self.bones, src=self.audio_path)\n",
    "\n",
    "        self.STATIC_FRAME = (metadata.num_frames / self.SAMPLE_RATE) / len(self.blendshapes)\n",
    "\n",
    "        for idx in range(len(self.blendshapes)):\n",
    "            approx_time = (idx + 1) * self.STATIC_FRAME\n",
    "            self.blendshapes.iloc[idx, 0] = approx_time\n",
    "            self.bones.iloc[idx, 0] = approx_time\n",
    "\n",
    "        if effects:\n",
    "            self.SPEECH_WAVEFORM, self.SAMPLE_RATE = torchaudio.sox_effects.apply_effects_tensor(self.SPEECH_WAVEFORM, self.SAMPLE_RATE, effects)\n",
    "\n",
    "        self.init_mfcc()\n",
    "\n",
    "    def init_mfcc(self):\n",
    "        def mfcc_transform(n_mfcc, n_mels, n_fft, win_length, hop_length):\n",
    "            print(\"MFCC with\")\n",
    "            print(\" - number of mfcc:\", n_mfcc)\n",
    "            print(\" - number of mels:\", n_mels)\n",
    "            print(\" - number of fft:\", n_fft)\n",
    "            print(\" - window length:\", win_length)\n",
    "            print(\" - hop length:\", hop_length)\n",
    "            return T.MFCC(\n",
    "                sample_rate=self.SAMPLE_RATE,\n",
    "                n_mfcc=n_mfcc,\n",
    "                melkwargs={\n",
    "                    \"n_fft\": n_fft,\n",
    "                    \"n_mels\": n_mels,\n",
    "                    \"win_length\": win_length,\n",
    "                    \"hop_length\": hop_length,\n",
    "                    \"window_fn\": torch.hann_window\n",
    "                },\n",
    "            )\n",
    "        \n",
    "        n_mfcc = self.n_mfcc\n",
    "        n_mels = n_mfcc * 2\n",
    "        n_fft = int(self.STATIC_FRAME * self.SAMPLE_RATE) // 16\n",
    "        win_length = n_fft\n",
    "        hop_length = n_fft // 2\n",
    "\n",
    "        self.mfcc = mfcc_transform(n_mfcc, n_mels, n_fft, win_length, hop_length)(self.SPEECH_WAVEFORM)\n",
    "        self.hop_length = hop_length\n",
    "        print(\" - number of mfcc frames:\", len(self.mfcc[0][0]))\n",
    "        print()\n",
    "\n",
    "    def input_dim(self):\n",
    "        return self.n_mfcc * self.n_vrmframes\n",
    "    \n",
    "    def output_dim(self):\n",
    "        return len(self.blendshapes.iloc[0]) + len(self.bones.iloc[0]) - 2\n",
    "\n",
    "    def _set_static_frame(self, _sf):\n",
    "        self.STATIC_FRAME = _sf\n",
    "        self.init_mfcc()\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.audio_only:\n",
    "            metadata = torchaudio.info(self.audio_path)\n",
    "            return int((metadata.num_frames / metadata.sample_rate) / self.STATIC_FRAME)\n",
    "        return len(self.blendshapes)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        # Number of frames before and after timestamp to get\n",
    "        n_vrmframes_half = self.n_vrmframes // 2\n",
    "\n",
    "        mfcc_index = min((idx + 1) * n_vrmframes_half, self.mfcc.shape[2] - n_vrmframes_half - 1)\n",
    "        mfcc_frame = torch.zeros(self.n_mfcc, self.n_vrmframes)\n",
    "        # print(idx, mfcc_index, n_vrmframes_half)\n",
    "        mfcc_frame[:] = self.mfcc[0,:,mfcc_index - n_vrmframes_half:mfcc_index + n_vrmframes_half]\n",
    "\n",
    "        if self.audio_only:\n",
    "            return mfcc_frame, torch.empty(1)\n",
    "                \n",
    "        time_window = self.blendshapes.iloc[idx, 0]\n",
    "\n",
    "        blendshape_params = self.blendshapes.iloc[idx, 1:]\n",
    "        blendshape_params = np.asarray(blendshape_params)\n",
    "        blendshape_params = blendshape_params.astype('float')\n",
    "\n",
    "        assert time_window == self.bones.iloc[idx, 0]\n",
    "\n",
    "        bone_params = self.bones.iloc[idx, 1:]\n",
    "        bone_params = np.asarray(bone_params)\n",
    "        bone_params = bone_params.astype('float')\n",
    "\n",
    "        return mfcc_frame, torch.Tensor(np.concatenate((blendshape_params, bone_params)))\n",
    "    \n",
    "\n",
    "effect = [[\"sinc\", \"300-3k\"]]\n",
    "effect = None # Windows\n",
    "\n",
    "train_file = \"8-8-2023 3-15-42 PM\"\n",
    "valid_file = \"2023-08-15 10-05-57 PM\"\n",
    "\n",
    "train = VRMParamsDataset(train_file, TRAINING_DATA_PATH, effect)\n",
    "valid = VRMParamsDataset(valid_file, TRAINING_DATA_PATH, effect, bs_scaler=train.bs_scaler, bone_scaler=train.bone_scaler)\n",
    "test = VRMParamsDataset(\"7-17-2023 4-53-18 PM\", TRAINING_DATA_PATH, effect, bs_scaler=train.bs_scaler, bone_scaler=train.bone_scaler)\n",
    "# test._set_static_frame(train.STATIC_FRAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VISUAL = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play audio of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (VISUAL):\n",
    "    Audio(train.SPEECH_WAVEFORM, rate=train.SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play audio of testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (VISUAL):\n",
    "    Audio(test.SPEECH_WAVEFORM, rate=test.SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Waveform of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (VISUAL):\n",
    "    plot_waveform(train.SPEECH_WAVEFORM, train.SAMPLE_RATE, title=\"Training audio\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([39, 3085715])\n"
     ]
    }
   ],
   "source": [
    "print(train.mfcc[0].shape)\n",
    "if (VISUAL):\n",
    "    plot_spectrogram(train.mfcc[0])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Waveform of testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (VISUAL):\n",
    "    plot_waveform(test.SPEECH_WAVEFORM, test.SAMPLE_RATE, title=\"Testing audio\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([39, 17246])\n"
     ]
    }
   ],
   "source": [
    "print(test.mfcc[0].shape)\n",
    "if (VISUAL):\n",
    "    plot_spectrogram(test.mfcc[0])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([39, 64]), torch.Size([396]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 50\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=False)\n",
    "valid_loader = DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "_x, _y = train[0]\n",
    "_x.shape, _y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VRMLoss(nn.Module):\n",
    "    bs_smooth_diff = 0.1\n",
    "    bone_smooth_diff = 0.4\n",
    "\n",
    "    def __init__(self, weights): #, non_expr_face_bs_weight=1.0):\n",
    "        super(VRMLoss, self).__init__()\n",
    "        self.bs_huber_weight = weights['bs']['huber']\n",
    "        self.bs_smooth_weight = weights['bs']['smooth']\n",
    "\n",
    "        self.bone_huber_weight = weights['bone']['huber']\n",
    "        self.bone_smooth_weight = weights['bone']['smooth']\n",
    "        \n",
    "        # self.non_expr_face_bs_weight = non_expr_face_bs_weight\n",
    "\n",
    "    def forward(self, output, target):\n",
    "\n",
    "        bs_output = output.squeeze()[:len(BLENDSHAPE_PARAMS)]\n",
    "        bs_target = target.squeeze()[:len(BLENDSHAPE_PARAMS)]\n",
    "\n",
    "        bone_output = output.squeeze()[len(BLENDSHAPE_PARAMS):]\n",
    "        bone_target = target.squeeze()[len(BLENDSHAPE_PARAMS):]\n",
    "        \n",
    "        # n_loss = torch.nn.MSELoss()(bs_output, bs_target)\n",
    "\n",
    "        # def expr_blendshapes(blendshapes):\n",
    "        #     params = ['A', 'Angry', 'E', 'Fun', 'I', 'Joy', 'Neutral', 'O', 'U', 'Sorrow', 'Surprised']\n",
    "        #     # print(blendshapes.shape)\n",
    "        #     return torch.cat(tuple([blendshapes[:,BLENDSHAPE_PARAMS.index(p)] for p in params]), dim=0)\n",
    "\n",
    "        # bs_output = expr_blendshapes(bs_output)\n",
    "        # bs_target = expr_blendshapes(bs_target)\n",
    "        \n",
    "        bs_prev = None\n",
    "        bs_smooth = 0.\n",
    "        \n",
    "        bone_prev = None\n",
    "        bone_smooth = 0.\n",
    "        \n",
    "        bs_huber = nn.HuberLoss()(bs_output, bs_target)\n",
    "        for sample in range(len(bs_target)):\n",
    "            # print(output[sample].shape, target[sample].shape)\n",
    "            # print(bs_output[sample].shape, bs_target[sample].shape)\n",
    "            bs_smooth += 0 if bs_prev is None else 1. - abs(self.bs_smooth_diff - nn.CosineSimilarity(dim=0)(bs_output[sample], bs_prev).mean())\n",
    "            bs_prev = bs_output[sample]\n",
    "        \n",
    "        bone_huber = nn.HuberLoss()(bone_output, bone_target)\n",
    "        for sample in range(len(bone_target)):\n",
    "            bone_smooth += 0 if bone_prev is None else 1. - abs(self.bone_smooth_diff - nn.CosineSimilarity(dim=0)(bone_output[sample], bone_prev).mean())\n",
    "            bone_prev = bone_output[sample]\n",
    "\n",
    "        def a2f_loss(huber, smooth, length, w=[1.0, 1.0]):\n",
    "            return (w[0]*huber + w[1]*smooth) / length\n",
    "        \n",
    "        return a2f_loss(bs_huber, bs_smooth, len(bs_target), [self.bs_huber_weight, self.bs_smooth_weight]) + a2f_loss(bone_huber, bone_smooth, len(bone_target), [self.bone_huber_weight, self.bone_smooth_weight]) # + self.non_expr_face_bs_weight * n_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, batch_size, output_dim, num_layers, p):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            input_dim: Input layer dimension\n",
    "            hidden_dim: Hidden layer dimension\n",
    "            batch_size: Batch size of data\n",
    "            output_dim: Output layer dimension\n",
    "            num_layers: Number of layers\n",
    "            p: Dropout\n",
    "        \"\"\"\n",
    "        super(BiLSTM, self).__init__()\n",
    "\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # self.init_linear = nn.Linear(self.input_dim, self.input_dim)\n",
    "        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, self.num_layers, batch_first=True, bidirectional=True, dropout=p)\n",
    "\n",
    "        self.mdn = mdn.MDN(self.hidden_dim * 2, self.hidden_dim * 2, 1)\n",
    "        self.linear_hidden = nn.Linear(self.hidden_dim * 2, self.hidden_dim)\n",
    "        \n",
    "        self.energy = nn.Linear(self.hidden_dim*3, 1)\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc = nn.Linear(self.hidden_dim*4, output_dim)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        w1 = torch.randn(self.num_layers, self.batch_size, self.hidden_dim)\n",
    "        w2 = torch.randn(self.num_layers, self.batch_size, self.hidden_dim)\n",
    "        return w1, w2\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = self.dropout(input)\n",
    "\n",
    "        # print('----------------------------------------------------------------------------------')\n",
    "        # print(input)\n",
    "        \n",
    "        lstm_out, (hidden, cell) = self.lstm(input)\n",
    "        # print('               LSTM')\n",
    "        # print(lstm_out)\n",
    "\n",
    "        # print('               MDN')\n",
    "        # print(mdn_out)\n",
    "\n",
    "        # print(hidden[0:2].shape)\n",
    "        \n",
    "        hidden = self.linear_hidden(hidden[0:2].reshape(1, -1, self.hidden_dim * 2)).permute(1, 0, 2)\n",
    "        # print('               HIDD')\n",
    "        # print(hidden)\n",
    "\n",
    "        attn = self.softmax(self.relu(self.energy(torch.cat((hidden, lstm_out), dim=2))))\n",
    "        # print('               ATTN')\n",
    "        # print(attn)\n",
    "        context = torch.bmm(attn, lstm_out).permute(1, 0, 2)\n",
    "        mdn_out = self.mdn(context)[2].permute(1, 0, 2)\n",
    "\n",
    "        # print(context.shape, lstm_out.shape)\n",
    "        y_pred = self.fc(torch.cat((mdn_out, lstm_out.permute(1, 0, 2)), dim=2)).squeeze()\n",
    "        # print('               PRED')\n",
    "        # print(y_pred)\n",
    "        return y_pred\n",
    "\n",
    "n_epochs = 3\n",
    "lr = 0.0001\n",
    "lstm_input_size = train.input_dim()\n",
    "hidden_state_size = 512\n",
    "num_sequence_layers = 2\n",
    "output_dim = train.output_dim()\n",
    "save_interval = 50\n",
    "dropoff = 0.02\n",
    "\n",
    "model = BiLSTM(lstm_input_size, hidden_state_size, batch_size, output_dim, num_sequence_layers, dropoff)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTM(\n",
       "  (dropout): Dropout(p=0.02, inplace=False)\n",
       "  (lstm): LSTM(2496, 512, num_layers=2, batch_first=True, dropout=0.02, bidirectional=True)\n",
       "  (mdn): MDN(\n",
       "    (pi): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=1, bias=True)\n",
       "      (1): Softmax(dim=1)\n",
       "    )\n",
       "    (sigma): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (mu): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  )\n",
       "  (linear_hidden): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (energy): Linear(in_features=1536, out_features=1, bias=True)\n",
       "  (softmax): Softmax(dim=0)\n",
       "  (relu): ReLU()\n",
       "  (fc): Linear(in_features=2048, out_features=396, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'bs': {\n",
    "        'huber': 9.0,\n",
    "        'smooth': 0.2\n",
    "    },\n",
    "    'bone': {\n",
    "        'huber': 1.0,\n",
    "        'smooth': 0.2\n",
    "    }\n",
    "}\n",
    "loss_fn = VRMLoss(weights)\n",
    "# valid_loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_state(state_dict, name, train_file, batch_size, epochs, lr, hidden_state_size, num_sequence_layers, weights, valid_loss):\n",
    "    torch.save(state_dict, 'models/{}.pt'.format(name))\n",
    "    with open('models/meta-{}.txt'.format(name), 'w') as f:\n",
    "        metastring = 'Train file: {}\\nBatch size: {}\\nNumber of epochs: {}\\nLearning rate: {}\\nHidden state size: {}\\nNumber of LSTM layers: {}\\nWeights: {}\\nValidation loss: {}'\n",
    "        f.write(metastring.format(train_file, batch_size, epochs, lr, hidden_state_size, num_sequence_layers, str(weights), valid_loss))\n",
    "\n",
    "def save_model(model, name, train_file, batch_size, epochs, lr, hidden_state_size, num_sequence_layers, weights, valid_loss):\n",
    "    save_model_state(model.state_dict(), name, train_file, batch_size, epochs, lr, hidden_state_size, num_sequence_layers, weights, valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 \tTrain loss=0.0987156145 \tValid loss=0.1011988965\tTime=109.99s\n",
      "Best model: Model at epoch 0 with valid loss 0.1011988965\n",
      "Epoch 2/50 \tTrain loss=0.0980066061 \tValid loss=0.1011967421\tTime=109.98s\n",
      "Best model: Model at epoch 1 with valid loss 0.1011967421\n",
      "Epoch 3/50 \tTrain loss=0.0980107297 \tValid loss=0.1012119357\tTime=109.15s\n"
     ]
    }
   ],
   "source": [
    "min_valid_loss = np.inf\n",
    "best_model = None\n",
    "best_model_epoch = -1\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    start_time = time.time()\n",
    "    train_loss = 0.0\n",
    "    model.train()\n",
    "    for i, (mfcc, vrm_params) in enumerate(train_loader):\n",
    "        mfcc = mfcc.to(device=device).reshape(-1, 1, lstm_input_size)\n",
    "        vrm_params = vrm_params.to(device=device).reshape(-1, 1, output_dim)\n",
    "\n",
    "        pred = model(mfcc)\n",
    "\n",
    "        loss = loss_fn(pred, vrm_params)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    train_loss /= len(train_loader)\n",
    "        \n",
    "    valid_loss = 0.0\n",
    "    model.eval()\n",
    "    for i, (mfcc, vrm_params) in enumerate(valid_loader):\n",
    "        mfcc = mfcc.to(device=device).reshape(-1, 1, lstm_input_size)\n",
    "        vrm_params = vrm_params.to(device=device).reshape(-1, 1, output_dim)\n",
    "        \n",
    "        pred = model(mfcc)\n",
    "        \n",
    "        loss = loss_fn(pred, vrm_params)\n",
    "        valid_loss += loss.item()\n",
    "\n",
    "    valid_loss /= len(valid_loader)\n",
    "\n",
    "    print('Epoch {}/{} \\tTrain loss={:.10f} \\tValid loss={:.10f}\\tTime={:.2f}s'.format(epoch + 1, n_epochs, train_loss, valid_loss, time.time() - start_time))\n",
    "\n",
    "    if ((epoch+1) % save_interval == 0):\n",
    "        save_model(model, 'sample-model-' + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%SZ\"), train_file, batch_size, epoch, lr, hidden_state_size, num_sequence_layers, weights, valid_loss)\n",
    "    \n",
    "    if min_valid_loss > valid_loss:\n",
    "        min_valid_loss = valid_loss\n",
    "        best_model = model.state_dict()\n",
    "        best_model_epoch = epoch\n",
    "\n",
    "        print('Best model: Model at epoch {} with valid loss {:.10f}'.format(best_model_epoch, min_valid_loss))\n",
    "\n",
    "print('Best model: Model at epoch {} with valid loss {:.10f}'.format(best_model_epoch, min_valid_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_file = 'model_' + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%SZ\")\n",
    "# Save latest model\n",
    "save_model(model, weights_file + '_LATEST', train_file, batch_size, n_epochs, lr, hidden_state_size, num_sequence_layers, weights, valid_loss)\n",
    "# Save best model\n",
    "save_model_state(best_model, weights_file + '_BEST', train_file, batch_size, best_model_epoch, lr, hidden_state_size, num_sequence_layers, weights, min_valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTM(lstm_input_size, hidden_state_size, batch_size, output_dim, num_sequence_layers, dropoff)\n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load('models/{}.pt'.format(weights_file + '_BEST'), map_location=device))\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = torch.zeros(output_dim * len(test))\n",
    "test_actual = torch.zeros(output_dim * len(test))\n",
    "\n",
    "for i, (mfcc, vrm_params) in enumerate(test_loader):\n",
    "    mfcc = mfcc.to(device=device).reshape(-1, 1, lstm_input_size)\n",
    "    if not test.audio_only:\n",
    "        vrm_params = vrm_params.cpu().reshape(-1)\n",
    "    y_pred = model(mfcc).cpu().detach().reshape(-1)\n",
    "    test_preds[i*batch_size*output_dim:(i+1)*batch_size*output_dim] = y_pred\n",
    "    if not test.audio_only:\n",
    "        test_actual[i*batch_size*output_dim:(i+1)*batch_size*output_dim] = vrm_params\n",
    "\n",
    "if not test.audio_only:\n",
    "    rmse = torch.sqrt(torch.nn.functional.mse_loss(test_preds, test_actual))\n",
    "    print('RMSE: {:.10f}'.format(rmse))\n",
    "        \n",
    "test_preds = test_preds.reshape(len(test), output_dim)\n",
    "test_actual = test_actual.reshape(len(test), output_dim)\n",
    "\n",
    "test_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTION_DATA_PATH = './sampledata/Prediction'\n",
    "\n",
    "def to_csv(test_preds, name='prediction'):\n",
    "    blendshape_params = pd.DataFrame(test_preds[:,:len(BLENDSHAPE_PARAMS)])\n",
    "    blendshape_params = pd.DataFrame(train.bs_scaler.inverse_transform(blendshape_params.values))\n",
    "    blendshape_params.columns = BLENDSHAPE_PARAMS\n",
    "    \n",
    "    bone_params = pd.DataFrame(test_preds[:,len(BLENDSHAPE_PARAMS):])\n",
    "    bone_params = pd.DataFrame(train.bone_scaler.inverse_transform(bone_params.values))\n",
    "    bone_params.columns = [bone + t for t in ('PosX', 'PosY', 'PosZ', 'RotX', 'RotY', 'RotZ', 'RotW') for bone in BONE_PARAMS]\n",
    "    \n",
    "    STATIC_FRAME = test.STATIC_FRAME\n",
    "    \n",
    "    time_column = pd.DataFrame({'Time': [float(i)*STATIC_FRAME for i in range(len(test_preds))]})\n",
    "    \n",
    "    blendshape_params = pd.concat([time_column, blendshape_params], axis=1)\n",
    "    bone_params = pd.concat([time_column, bone_params], axis=1)\n",
    "    \n",
    "    blendshape_params.to_csv(PREDICTION_DATA_PATH + '/Blendshapes/' + name + '.csv', index=False)\n",
    "    bone_params.to_csv(PREDICTION_DATA_PATH + '/Bones/' + name + '.csv', index=False)\n",
    "    \n",
    "    shutil.copyfile(test.DATA_PATH + '/Audio/' + test.name + '.wav', PREDICTION_DATA_PATH + '/Audio/' + name + '.wav')\n",
    "\n",
    "datestring = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%SZ\")\n",
    "to_csv(test_preds, 'prediction-' + datestring)\n",
    "if not test.audio_only:\n",
    "    to_csv(test_actual, 'actual-' + datestring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
