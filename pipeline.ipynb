{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sing2Ani Training Pipeline\n",
    "\n",
    "### Dataset Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soundfile\n",
      "cuda:0\n",
      "2.0.1+cu118\n",
      "2.0.2+cu118\n",
      "----------\n",
      "Source: ./sampledata/Training/Audio/8-8-2023 3-15-42 PM.wav\n",
      "----------\n",
      " - sample_rate: 48000\n",
      " - num_channels: 1\n",
      " - num_frames: 86400000\n",
      " - bits_per_sample: 16\n",
      " - encoding: PCM_S\n",
      " - duration: 1800.0s\n",
      " - num_vrm_frames: 95449\n",
      " - vrm fps: 53.02722222222222\n",
      " - frames_per_vrm: 905.195444687739\n",
      " - seconds_per_vrm: 0.018858238430994562s\n",
      "\n",
      "MFCC with\n",
      " - number of mfcc: 39\n",
      " - number of mels: 78\n",
      " - number of fft: 56\n",
      " - window length: 56\n",
      " - hop length: 28\n",
      " - number of mfcc frames: 3085715\n",
      "\n",
      "----------\n",
      "Source: ./sampledata/Training/Audio/7-17-2023 4-58-09 PM.wav\n",
      "----------\n",
      " - sample_rate: 48000\n",
      " - num_channels: 1\n",
      " - num_frames: 2880000\n",
      " - bits_per_sample: 16\n",
      " - encoding: PCM_S\n",
      " - duration: 60.0s\n",
      " - num_vrm_frames: 539\n",
      " - vrm fps: 8.983333333333333\n",
      " - frames_per_vrm: 5343.228200371058\n",
      " - seconds_per_vrm: 0.11131725417439703s\n",
      "\n",
      "MFCC with\n",
      " - number of mfcc: 39\n",
      " - number of mels: 78\n",
      " - number of fft: 333\n",
      " - window length: 333\n",
      " - hop length: 166\n",
      " - number of mfcc frames: 17350\n",
      "\n",
      "----------\n",
      "Source: ./sampledata/Training/Audio/7-17-2023 4-53-18 PM.wav\n",
      "----------\n",
      " - sample_rate: 48000\n",
      " - num_channels: 1\n",
      " - num_frames: 2880000\n",
      " - bits_per_sample: 16\n",
      " - encoding: PCM_S\n",
      " - duration: 60.0s\n",
      " - num_vrm_frames: 537\n",
      " - vrm fps: 8.95\n",
      " - frames_per_vrm: 5363.128491620112\n",
      " - seconds_per_vrm: 0.11173184357541899s\n",
      "\n",
      "MFCC with\n",
      " - number of mfcc: 39\n",
      " - number of mels: 78\n",
      " - number of fft: 335\n",
      " - window length: 335\n",
      " - hop length: 167\n",
      " - number of mfcc frames: 17246\n",
      "\n",
      "----------\n",
      "Source: ./sampledata/Training/Audio/7-17-2023 4-58-09 PM.wav\n",
      "----------\n",
      " - sample_rate: 48000\n",
      " - num_channels: 1\n",
      " - num_frames: 2880000\n",
      " - bits_per_sample: 16\n",
      " - encoding: PCM_S\n",
      " - duration: 60.0s\n",
      " - num_vrm_frames: 539\n",
      " - vrm fps: 8.983333333333333\n",
      " - frames_per_vrm: 5343.228200371058\n",
      " - seconds_per_vrm: 0.11131725417439703s\n",
      "\n",
      "MFCC with\n",
      " - number of mfcc: 39\n",
      " - number of mels: 78\n",
      " - number of fft: 333\n",
      " - window length: 333\n",
      " - hop length: 166\n",
      " - number of mfcc frames: 17350\n",
      "\n",
      "----------\n",
      "Source: ./sampledata/Training/Audio/2023-08-15 10-05-57 PM.wav\n",
      "----------\n",
      " - sample_rate: 48000\n",
      " - num_channels: 1\n",
      " - num_frames: 8640000\n",
      " - bits_per_sample: 16\n",
      " - encoding: PCM_S\n",
      " - duration: 180.0s\n",
      " - num_vrm_frames: 12343\n",
      " - vrm fps: 68.57222222222222\n",
      " - frames_per_vrm: 699.9918982419185\n",
      " - seconds_per_vrm: 0.014583164546706636s\n",
      "\n",
      "MFCC with\n",
      " - number of mfcc: 39\n",
      " - number of mels: 78\n",
      " - number of fft: 43\n",
      " - window length: 43\n",
      " - hop length: 21\n",
      " - number of mfcc frames: 411429\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, 'pytorch-mdn/mdn')\n",
    "import mdn\n",
    "\n",
    "from pydub import AudioSegment\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "import torchaudio.functional as F\n",
    "import torchaudio.transforms as T\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "print(torchaudio.get_audio_backend())\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchaudio.__version__)\n",
    "\n",
    "from IPython.display import Audio\n",
    "from torchaudio.utils import download_asset\n",
    "\n",
    "# torch.random.manual_seed(0)\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()   # interactive mode\n",
    "\n",
    "TRAINING_DATA_PATH = \"./sampledata/Training\" # \"C:/Users/Kevin/AppData/LocalLow/kevinjycui/Training\"\n",
    "TESTING_DATA_PATH = \"./sampledata/Testing\" # \"C:/Users/Kevin/AppData/LocalLow/kevinjycui/Testing\"\n",
    "\n",
    "# VMC Protocol Standard\n",
    "BLENDSHAPE_PARAMS = [\"A\", \"Angry\", \"Blink\", \"Blink_L\", \"Blink_R\", \"E\", \"Fun\", \"I\", \"Joy\", \"LookDown\", \"LookLeft\", \"LookRight\", \"LookUp\", \"Neutral\", \"O\", \"Sorrow\", \"Surprised\", \"U\"]\n",
    "BONE_PARAMS = [\"Chest\", \"Head\", \"Hips\", \"LeftEye\", \"LeftFoot\", \"LeftHand\", \"LeftIndexDistal\", \"LeftIndexIntermediate\", \"LeftIndexProximal\", \"LeftLittleDistal\", \"LeftLittleIntermediate\", \"LeftLittleProximal\", \"LeftLowerArm\", \"LeftLowerLeg\", \"LeftMiddleDistal\", \"LeftMiddleIntermediate\", \"LeftMiddleProximal\", \"LeftRingDistal\", \"LeftRingIntermediate\", \"LeftRingProximal\", \"LeftShoulder\", \"LeftThumbDistal\", \"LeftThumbIntermediate\", \"LeftThumbProximal\", \"LeftToes\", \"LeftUpperArm\", \"LeftUpperLeg\", \"Neck\", \"RightEye\", \"RightFoot\", \"RightHand\", \"RightIndexDistal\", \"RightIndexIntermediate\", \"RightIndexProximal\", \"RightLittleDistal\", \"RightLittleIntermediate\", \"RightLittleProximal\", \"RightLowerArm\", \"RightLowerLeg\", \"RightMiddleDistal\", \"RightMiddleIntermediate\", \"RightMiddleProximal\", \"RightRingDistal\", \"RightRingIntermediate\", \"RightRingProximal\", \"RightShoulder\", \"RightThumbDistal\", \"RightThumbIntermediate\", \"RightThumbProximal\", \"RightToes\", \"RightUpperArm\", \"RightUpperLeg\", \"Spine\", \"UpperChest\"]\n",
    "EXPR_BS_PARAMS = [\"A\", \"Blink\", \"E\", \"I\", \"O\", \"U\"]\n",
    "\n",
    "def print_metadata(metadata, vrm, src=None):\n",
    "    if src:\n",
    "        print(\"-\" * 10)\n",
    "        print(\"Source:\", src)\n",
    "        print(\"-\" * 10)\n",
    "    print(\" - sample_rate:\", metadata.sample_rate)\n",
    "    print(\" - num_channels:\", metadata.num_channels)\n",
    "    print(\" - num_frames:\", metadata.num_frames)\n",
    "    print(\" - bits_per_sample:\", metadata.bits_per_sample)\n",
    "    print(\" - encoding:\", metadata.encoding)\n",
    "    duration = metadata.num_frames / metadata.sample_rate\n",
    "    print(\" - duration:\", duration, end='s\\n')\n",
    "    print(\" - num_vrm_frames:\", len(vrm))\n",
    "    print(\" - vrm fps:\", len(vrm) / duration)\n",
    "    frames_per_vrm = metadata.num_frames / len(vrm)\n",
    "    print(\" - frames_per_vrm:\", frames_per_vrm)\n",
    "    seconds_per_vrm = duration / len(vrm)\n",
    "    print(\" - seconds_per_vrm:\", seconds_per_vrm, end='s\\n')\n",
    "    print()\n",
    "\n",
    "def plot_waveform(waveform, sr, title=\"Waveform\"):\n",
    "    waveform = waveform.numpy()\n",
    "\n",
    "    num_channels, num_frames = waveform.shape\n",
    "    time_axis = torch.arange(0, num_frames) / sr\n",
    "\n",
    "    figure, axes = plt.subplots(num_channels, 1)\n",
    "    if num_channels == 1:\n",
    "        axes.plot(time_axis, waveform[0], linewidth=1)\n",
    "        axes.grid(True)\n",
    "    else:\n",
    "        for axis in axes:\n",
    "            axis.plot(time_axis, waveform[0], linewidth=1)\n",
    "            axis.grid(True)\n",
    "    figure.suptitle(title)\n",
    "\n",
    "def plot_spectrogram(specgram, title=None, ylabel=\"freq_bin\"):\n",
    "    fig, axs = plt.subplots(1, 1)\n",
    "    axs.set_title(title or \"Spectrogram (db)\")\n",
    "    axs.set_ylabel(ylabel)\n",
    "    axs.set_xlabel(\"frame\")\n",
    "    im = axs.imshow(librosa.power_to_db(specgram), origin=\"lower\", aspect=\"auto\")\n",
    "    fig.colorbar(im, ax=axs)\n",
    "\n",
    "class VRMParamsDataset(Dataset):\n",
    "    \"\"\" VRM Parameter Dataset \"\"\"\n",
    "\n",
    "    n_mfcc = 39\n",
    "    n_fft_per_vrm = 16\n",
    "    n_vrmframes = 64\n",
    "\n",
    "    def __init__(self, filename, DATA_PATH=TRAINING_DATA_PATH, effects=None, audio_only=False, scaler=None):\n",
    "        \"\"\"\n",
    "            filename (string): Path to wav/csv files.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.audio_only = audio_only\n",
    "        self.name = Path(filename).stem\n",
    "        self.DATA_PATH = DATA_PATH\n",
    "\n",
    "        self.audio_path = self.DATA_PATH + f\"/Audio/{self.name}.wav\"\n",
    "\n",
    "        self.SPEECH_WAVEFORM, self.SAMPLE_RATE = torchaudio.load(self.audio_path)\n",
    "        self.SPEECH_WAVEFORM = torch.mean(self.SPEECH_WAVEFORM, dim=0).unsqueeze(0)\n",
    "\n",
    "        if self.audio_only:\n",
    "            return\n",
    "\n",
    "        blendshapes = pd.read_csv(DATA_PATH + \"/Blendshapes/\" + self.name + \".csv\")\n",
    "        blendshapes.columns = ['Time'] + BLENDSHAPE_PARAMS\n",
    "        for param in BLENDSHAPE_PARAMS:\n",
    "            if param not in EXPR_BS_PARAMS:\n",
    "                blendshapes = blendshapes.drop(param, axis=1)\n",
    "        \n",
    "        bones = pd.read_csv(DATA_PATH + \"/Bones/\" + self.name + \".csv\")\n",
    "\n",
    "        # Combine blendshape and bone data\n",
    "        self.data = pd.concat([blendshapes, bones.iloc[:, 1:]], axis=1)\n",
    "\n",
    "        metadata = torchaudio.info(self.audio_path)\n",
    "        print_metadata(metadata, self.data, src=self.audio_path)\n",
    "\n",
    "        # Set static time window for each row\n",
    "        self.STATIC_FRAME = (metadata.num_frames / self.SAMPLE_RATE) / len(self.data)\n",
    "\n",
    "        for idx in range(len(self.data)):\n",
    "            approx_time = (idx + 1) * self.STATIC_FRAME\n",
    "            self.data.iloc[idx, 0] = approx_time\n",
    "\n",
    "        # Normalize data\n",
    "        self.scaler = scaler\n",
    "        if self.scaler == None:\n",
    "            self.scaler = MinMaxScaler(feature_range=(0, 1)).fit(self.data.iloc[:, 1:])\n",
    "\n",
    "        data_scaled = self.scaler.transform(self.data.iloc[:, 1:].values)\n",
    "        data_scaled = pd.DataFrame(data_scaled)\n",
    "        self.data = pd.concat([self.data.iloc[:, 0], data_scaled], axis=1)\n",
    "\n",
    "        # Apply effects such as noise reduction\n",
    "        if effects:\n",
    "            self.SPEECH_WAVEFORM, self.SAMPLE_RATE = torchaudio.sox_effects.apply_effects_tensor(self.SPEECH_WAVEFORM, self.SAMPLE_RATE, effects)\n",
    "\n",
    "        self.init_mfcc()\n",
    "\n",
    "    def init_mfcc(self):\n",
    "        '''\n",
    "        Create MFCC from audio\n",
    "        '''\n",
    "        def mfcc_transform(n_mfcc, n_mels, n_fft, win_length, hop_length):\n",
    "            print(\"MFCC with\")\n",
    "            print(\" - number of mfcc:\", n_mfcc)\n",
    "            print(\" - number of mels:\", n_mels)\n",
    "            print(\" - number of fft:\", n_fft)\n",
    "            print(\" - window length:\", win_length)\n",
    "            print(\" - hop length:\", hop_length)\n",
    "            return T.MFCC(\n",
    "                sample_rate=self.SAMPLE_RATE,\n",
    "                n_mfcc=n_mfcc,\n",
    "                melkwargs={\n",
    "                    \"n_fft\": n_fft,\n",
    "                    \"n_mels\": n_mels,\n",
    "                    \"win_length\": win_length,\n",
    "                    \"hop_length\": hop_length,\n",
    "                    \"window_fn\": torch.hann_window\n",
    "                },\n",
    "            )\n",
    "        # Number of MFCC\n",
    "        n_mfcc = self.n_mfcc\n",
    "        n_mels = n_mfcc * 2\n",
    "        # FFT is every nth of a window, allows for n*2 FFTs per VRM/VMC with potential overlap\n",
    "        n_fft = int(self.STATIC_FRAME * self.SAMPLE_RATE) // self.n_fft_per_vrm\n",
    "        win_length = n_fft\n",
    "        hop_length = n_fft // 2\n",
    "\n",
    "        self.mfcc = mfcc_transform(n_mfcc, n_mels, n_fft, win_length, hop_length)(self.SPEECH_WAVEFORM)\n",
    "        self.hop_length = hop_length\n",
    "        print(\" - number of mfcc frames:\", len(self.mfcc[0][0]))\n",
    "        print()\n",
    "\n",
    "    def input_dim(self):\n",
    "        return self.n_mfcc * self.n_vrmframes\n",
    "    \n",
    "    def output_dim(self):\n",
    "        return len(self.data.iloc[0]) - 1\n",
    "\n",
    "    def _set_static_frame(self, _sf):\n",
    "        self.STATIC_FRAME = _sf\n",
    "        self.init_mfcc()\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.audio_only:\n",
    "            metadata = torchaudio.info(self.audio_path)\n",
    "            return int((metadata.num_frames / metadata.sample_rate) / self.STATIC_FRAME)\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        # Number of frames before and after timestamp to get\n",
    "        n_vrmframes_half = self.n_vrmframes // 2\n",
    "\n",
    "        mfcc_index = min((idx + 1) * n_vrmframes_half, self.mfcc.shape[2] - n_vrmframes_half - 1)\n",
    "        mfcc_frame = torch.zeros(self.n_mfcc, self.n_vrmframes)\n",
    "        # print(idx, mfcc_index, n_vrmframes_half)\n",
    "        mfcc_frame[:] = self.mfcc[0,:,mfcc_index - n_vrmframes_half:mfcc_index + n_vrmframes_half]\n",
    "\n",
    "        if self.audio_only:\n",
    "            return mfcc_frame, torch.empty(1)\n",
    "                \n",
    "        time_window = self.data.iloc[idx, 0]\n",
    "\n",
    "        vrm_params = self.data.iloc[idx, 1:]\n",
    "        vrm_params = np.asarray(vrm_params)\n",
    "        vrm_params = vrm_params.astype('float')\n",
    "\n",
    "        return mfcc_frame, torch.Tensor(vrm_params)\n",
    "    \n",
    "\n",
    "effect = [[\"sinc\", \"300-3k\"]]\n",
    "effect = None # libsox not available for Windows\n",
    "\n",
    "train_file = [\"8-8-2023 3-15-42 PM\"]\n",
    "valid_file = \"7-17-2023 4-58-09 PM\"\n",
    "\n",
    "train0 = VRMParamsDataset(train_file[0], TRAINING_DATA_PATH, effect)\n",
    "train = [VRMParamsDataset(file, TRAINING_DATA_PATH, effect, scaler=train0.scaler) for file in train_file[1:]]\n",
    "train.insert(0, train0)\n",
    "assert all([t.input_dim() == train0.input_dim() and t.output_dim() == train0.output_dim() for t in train])\n",
    "\n",
    "valid = VRMParamsDataset(valid_file, TRAINING_DATA_PATH, effect, scaler=train0.scaler)\n",
    "test = VRMParamsDataset(\"7-17-2023 4-53-18 PM\", TRAINING_DATA_PATH, effect, scaler=train0.scaler)\n",
    "# test._set_static_frame(train.STATIC_FRAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot waveforms and spectrograms\n",
    "VISUAL = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play audio of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (VISUAL):\n",
    "    Audio(train[0].SPEECH_WAVEFORM, rate=train[0].SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play audio of testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (VISUAL):\n",
    "    Audio(test.SPEECH_WAVEFORM, rate=test.SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Waveform of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (VISUAL):\n",
    "    plot_waveform(train[0].SPEECH_WAVEFORM, train.SAMPLE_RATE, title=\"Training audio\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([39, 3085715])\n"
     ]
    }
   ],
   "source": [
    "print(train[0].mfcc[0].shape)\n",
    "if (VISUAL):\n",
    "    plot_spectrogram(train[0].mfcc[0])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Waveform of testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (VISUAL):\n",
    "    plot_waveform(test.SPEECH_WAVEFORM, test.SAMPLE_RATE, title=\"Testing audio\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([39, 411429])\n"
     ]
    }
   ],
   "source": [
    "print(test.mfcc[0].shape)\n",
    "if (VISUAL):\n",
    "    plot_spectrogram(test.mfcc[0])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([39, 64]), torch.Size([384]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 100\n",
    "\n",
    "train_loader = [DataLoader(t, batch_size=batch_size, shuffle=False) for t in train]\n",
    "valid_loader = DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "_x, _y = train[0][0]\n",
    "_x.shape, _y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VRMLoss(nn.Module):\n",
    "    bs_smooth_diff = 0.7\n",
    "    bone_smooth_diff = 0.3\n",
    "\n",
    "    def __init__(self, weights): #, non_expr_face_bs_weight=1.0):\n",
    "        super(VRMLoss, self).__init__()\n",
    "        self.bs_huber_weight = weights['bs']['huber']\n",
    "        self.bs_smooth_weight = weights['bs']['smooth']\n",
    "\n",
    "        self.bone_huber_weight = weights['bone']['huber']\n",
    "        self.bone_smooth_weight = weights['bone']['smooth']\n",
    "        \n",
    "        # self.non_expr_face_bs_weight = non_expr_face_bs_weight\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        # Separate data into blendshape and bone\n",
    "        bs_output = output.squeeze()[:len(BLENDSHAPE_PARAMS)]\n",
    "        bs_target = target.squeeze()[:len(BLENDSHAPE_PARAMS)]\n",
    "\n",
    "        bone_output = output.squeeze()[len(BLENDSHAPE_PARAMS):]\n",
    "        bone_target = target.squeeze()[len(BLENDSHAPE_PARAMS):]\n",
    "        \n",
    "        # n_loss = torch.nn.MSELoss()(bs_output, bs_target)\n",
    "        \n",
    "        bs_prev = None\n",
    "        bs_smooth = 0.\n",
    "        \n",
    "        bone_prev = None\n",
    "        bone_smooth = 0.\n",
    "\n",
    "        bs_huber = nn.HuberLoss()(bs_output, bs_target)\n",
    "        for sample in range(len(bs_target)):\n",
    "            # print(output[sample].shape, target[sample].shape)\n",
    "            # print(bs_output[sample].shape, bs_target[sample].shape)\n",
    "            bs_smooth += 0 if bs_prev is None else 1. - abs(self.bs_smooth_diff - nn.CosineSimilarity(dim=0)(bs_output[sample], bs_prev).mean())\n",
    "            bs_prev = bs_output[sample]\n",
    "        \n",
    "        bone_huber = nn.HuberLoss()(bone_output, bone_target)\n",
    "        for sample in range(len(bone_target)):\n",
    "            bone_smooth += 0 if bone_prev is None else 1. - abs(self.bone_smooth_diff - nn.CosineSimilarity(dim=0)(bone_output[sample], bone_prev).mean())\n",
    "            bone_prev = bone_output[sample]\n",
    "\n",
    "        def a2f_loss(huber, smooth, length, w=[1.0, 1.0]):\n",
    "            # Calculate loss as in Audio2Face (Guanzhong Tian; Yi Yuan; Yong Liu)\n",
    "            return (w[0]*huber + w[1]*smooth) / length\n",
    "\n",
    "        # Loss is computed separately for blendshape and bone with different weights and combined\n",
    "        return a2f_loss(bs_huber, bs_smooth, len(bs_target), [self.bs_huber_weight, self.bs_smooth_weight]) + a2f_loss(bone_huber, bone_smooth, len(bone_target), [self.bone_huber_weight, self.bone_smooth_weight]) # + self.non_expr_face_bs_weight * n_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, batch_size, output_dim, num_layers, p):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            input_dim: Input layer dimension\n",
    "            hidden_dim: Hidden layer dimension\n",
    "            batch_size: Batch size of data\n",
    "            output_dim: Output layer dimension\n",
    "            num_layers: Number of layers\n",
    "            p: Dropout\n",
    "        \"\"\"\n",
    "        super(BiLSTM, self).__init__()\n",
    "\n",
    "        # Dropout to prevent overfitting\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # self.init_linear = nn.Linear(self.input_dim, self.input_dim)\n",
    "\n",
    "        # Bidirectional LSTM to predict sequence with memory\n",
    "        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, self.num_layers, batch_first=True, bidirectional=True, dropout=p)\n",
    "\n",
    "        # Mixed Density Network layer to prevent sequence from staying still, outputs a probability density function\n",
    "        self.mdn = mdn.MDN(self.hidden_dim * 2, self.hidden_dim * 2, 1)\n",
    "\n",
    "        # Attention layer to determine important parameters from hidden weights\n",
    "        self.linear_hidden = nn.Linear(self.hidden_dim * 2, self.hidden_dim)\n",
    "        self.energy = nn.Linear(self.hidden_dim*3, 1)\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Fully connected layer to produce output\n",
    "        self.fc = nn.Linear(self.hidden_dim*4, output_dim)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # Hidden layers of LSTM initialised as Gaussian distribution\n",
    "        w1 = torch.randn(self.num_layers, self.batch_size, self.hidden_dim)\n",
    "        w2 = torch.randn(self.num_layers, self.batch_size, self.hidden_dim)\n",
    "        return w1, w2\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = self.dropout(input)\n",
    "\n",
    "        # print('----------------------------------------------------------------------------------')\n",
    "        # print(input)\n",
    "        \n",
    "        lstm_out, (hidden, cell) = self.lstm(input)\n",
    "        # print('               LSTM')\n",
    "        # print(lstm_out)\n",
    "\n",
    "        # print('               MDN')\n",
    "        # print(mdn_out)\n",
    "\n",
    "        # print(hidden[0:2].shape)\n",
    "        \n",
    "        hidden = self.linear_hidden(hidden[0:2].reshape(1, -1, self.hidden_dim * 2)).permute(1, 0, 2)\n",
    "        # print('               HIDD')\n",
    "        # print(hidden)\n",
    "\n",
    "        attn = self.softmax(self.relu(self.energy(torch.cat((hidden, lstm_out), dim=2))))\n",
    "        # print('               ATTN')\n",
    "        # print(attn)\n",
    "        context = torch.bmm(attn, lstm_out).permute(1, 0, 2)\n",
    "        mdn_out = self.mdn(context)[2].permute(1, 0, 2)\n",
    "\n",
    "        # print(context.shape, lstm_out.shape)\n",
    "        y_pred = self.fc(torch.cat((mdn_out, lstm_out.permute(1, 0, 2)), dim=2)).squeeze()\n",
    "        # print('               PRED')\n",
    "        # print(y_pred)\n",
    "        return y_pred\n",
    "\n",
    "# Training hyperparameters\n",
    "n_epochs = 50\n",
    "lr = 0.0001\n",
    "lstm_input_size = train[0].input_dim()\n",
    "hidden_state_size = 512\n",
    "num_sequence_layers = 2\n",
    "output_dim = train[0].output_dim()\n",
    "save_interval = 5\n",
    "dropoff = 0.01\n",
    "\n",
    "model = BiLSTM(lstm_input_size, hidden_state_size, batch_size, output_dim, num_sequence_layers, dropoff)\n",
    "model = model.to(device)\n",
    "\n",
    "# Adam optimizer (gradient descent)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('models/model_2023-08-17_15-50-19Z_LATEST.pt', map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'bs': {\n",
    "        'huber': 6.0,\n",
    "        'smooth': 3.5\n",
    "    },\n",
    "    'bone': {\n",
    "        'huber': 0.3,\n",
    "        'smooth': 0.1\n",
    "    }\n",
    "}\n",
    "loss_fn = VRMLoss(weights)\n",
    "valid_loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_state(state_dict, name, train_file, batch_size, epochs, lr, hidden_state_size, num_sequence_layers, weights, valid_loss):\n",
    "    torch.save(state_dict, 'models/{}.pt'.format(name))\n",
    "    with open('models/meta-{}.txt'.format(name), 'w') as f:\n",
    "        metastring = 'Train file: {}\\nBatch size: {}\\nNumber of epochs: {}\\nLearning rate: {}\\nHidden state size: {}\\nNumber of LSTM layers: {}\\nWeights: {}\\nValidation loss: {}'\n",
    "        f.write(metastring.format(train_file, batch_size, epochs, lr, hidden_state_size, num_sequence_layers, str(weights), valid_loss))\n",
    "\n",
    "def save_model(model, name, train_file, batch_size, epochs, lr, hidden_state_size, num_sequence_layers, weights, valid_loss):\n",
    "    save_model_state(model.state_dict(), name, train_file, batch_size, epochs, lr, hidden_state_size, num_sequence_layers, weights, valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 \tTrain loss=2.3491334850 \tValid loss=2.3463996649\tTime=93.25s\n",
      "Best model: Model at epoch 1 with valid loss 2.3463996649\n",
      "Epoch 2/50 \tTrain loss=2.3447373076 \tValid loss=2.3455907106\tTime=92.59s\n",
      "Best model: Model at epoch 2 with valid loss 2.3455907106\n",
      "Epoch 3/50 \tTrain loss=2.3446904035 \tValid loss=2.3451747894\tTime=93.79s\n",
      "Best model: Model at epoch 3 with valid loss 2.3451747894\n",
      "Epoch 4/50 \tTrain loss=2.3446646333 \tValid loss=2.3449686368\tTime=93.57s\n",
      "Best model: Model at epoch 4 with valid loss 2.3449686368\n",
      "Epoch 5/50 \tTrain loss=2.3446507559 \tValid loss=2.3448470831\tTime=94.02s\n",
      "Best model: Model at epoch 5 with valid loss 2.3448470831\n",
      "Epoch 6/50 \tTrain loss=2.3446436907 \tValid loss=2.3447742065\tTime=94.03s\n",
      "Best model: Model at epoch 6 with valid loss 2.3447742065\n",
      "Epoch 7/50 \tTrain loss=2.3446412161 \tValid loss=2.3447320064\tTime=93.52s\n",
      "Best model: Model at epoch 7 with valid loss 2.3447320064\n",
      "Epoch 8/50 \tTrain loss=2.3446410579 \tValid loss=2.3447022438\tTime=93.18s\n",
      "Best model: Model at epoch 8 with valid loss 2.3447022438\n",
      "Epoch 9/50 \tTrain loss=2.3446418225 \tValid loss=2.3446815411\tTime=93.71s\n",
      "Best model: Model at epoch 9 with valid loss 2.3446815411\n",
      "Epoch 10/50 \tTrain loss=2.3446427353 \tValid loss=2.3446690241\tTime=94.04s\n",
      "Best model: Model at epoch 10 with valid loss 2.3446690241\n",
      "Epoch 11/50 \tTrain loss=2.3446434955 \tValid loss=2.3446600835\tTime=93.13s\n",
      "Best model: Model at epoch 11 with valid loss 2.3446600835\n",
      "Epoch 12/50 \tTrain loss=2.3446441209 \tValid loss=2.3446535667\tTime=92.91s\n",
      "Best model: Model at epoch 12 with valid loss 2.3446535667\n",
      "Epoch 13/50 \tTrain loss=2.3446444898 \tValid loss=2.3446491559\tTime=93.98s\n",
      "Best model: Model at epoch 13 with valid loss 2.3446491559\n",
      "Epoch 14/50 \tTrain loss=2.3446447283 \tValid loss=2.3446462552\tTime=93.39s\n",
      "Best model: Model at epoch 14 with valid loss 2.3446462552\n",
      "Epoch 15/50 \tTrain loss=2.3446449282 \tValid loss=2.3446438710\tTime=93.17s\n",
      "Best model: Model at epoch 15 with valid loss 2.3446438710\n",
      "Epoch 16/50 \tTrain loss=2.3446451674 \tValid loss=2.3446416060\tTime=93.86s\n",
      "Best model: Model at epoch 16 with valid loss 2.3446416060\n",
      "Epoch 17/50 \tTrain loss=2.3446454780 \tValid loss=2.3446397781\tTime=93.47s\n",
      "Best model: Model at epoch 17 with valid loss 2.3446397781\n",
      "Epoch 18/50 \tTrain loss=2.3446456712 \tValid loss=2.3446386258\tTime=93.48s\n",
      "Best model: Model at epoch 18 with valid loss 2.3446386258\n",
      "Epoch 19/50 \tTrain loss=2.3446457236 \tValid loss=2.3446377118\tTime=93.41s\n",
      "Best model: Model at epoch 19 with valid loss 2.3446377118\n",
      "Epoch 20/50 \tTrain loss=2.3446457696 \tValid loss=2.3446368774\tTime=94.01s\n",
      "Best model: Model at epoch 20 with valid loss 2.3446368774\n",
      "Epoch 21/50 \tTrain loss=2.3446457880 \tValid loss=2.3446362813\tTime=93.40s\n",
      "Best model: Model at epoch 21 with valid loss 2.3446362813\n",
      "Epoch 22/50 \tTrain loss=2.3446458442 \tValid loss=2.3446356058\tTime=93.37s\n",
      "Best model: Model at epoch 22 with valid loss 2.3446356058\n",
      "Epoch 23/50 \tTrain loss=2.3446460160 \tValid loss=2.3446341753\tTime=93.38s\n",
      "Best model: Model at epoch 23 with valid loss 2.3446341753\n",
      "Epoch 24/50 \tTrain loss=2.3446462189 \tValid loss=2.3446333806\tTime=93.05s\n",
      "Best model: Model at epoch 24 with valid loss 2.3446333806\n",
      "Epoch 25/50 \tTrain loss=2.3446463785 \tValid loss=2.3446329435\tTime=94.23s\n",
      "Best model: Model at epoch 25 with valid loss 2.3446329435\n",
      "Epoch 26/50 \tTrain loss=2.3446465315 \tValid loss=2.3446318309\tTime=93.08s\n",
      "Best model: Model at epoch 26 with valid loss 2.3446318309\n",
      "Epoch 27/50 \tTrain loss=2.3446466698 \tValid loss=2.3446292480\tTime=93.17s\n",
      "Best model: Model at epoch 27 with valid loss 2.3446292480\n",
      "Epoch 28/50 \tTrain loss=2.3446467876 \tValid loss=2.3446266254\tTime=92.89s\n",
      "Best model: Model at epoch 28 with valid loss 2.3446266254\n",
      "Epoch 29/50 \tTrain loss=2.3446468885 \tValid loss=2.3446241220\tTime=93.23s\n",
      "Best model: Model at epoch 29 with valid loss 2.3446241220\n",
      "Epoch 30/50 \tTrain loss=2.3446470423 \tValid loss=2.3446216186\tTime=93.10s\n",
      "Best model: Model at epoch 30 with valid loss 2.3446216186\n",
      "Epoch 31/50 \tTrain loss=2.3446471604 \tValid loss=2.3446191947\tTime=94.45s\n",
      "Best model: Model at epoch 31 with valid loss 2.3446191947\n",
      "Epoch 32/50 \tTrain loss=2.3446472493 \tValid loss=2.3446166118\tTime=92.83s\n",
      "Best model: Model at epoch 32 with valid loss 2.3446166118\n",
      "Epoch 33/50 \tTrain loss=2.3446473069 \tValid loss=2.3446148634\tTime=93.51s\n",
      "Best model: Model at epoch 33 with valid loss 2.3446148634\n",
      "Epoch 34/50 \tTrain loss=2.3446473671 \tValid loss=2.3446135918\tTime=93.21s\n",
      "Best model: Model at epoch 34 with valid loss 2.3446135918\n",
      "Epoch 35/50 \tTrain loss=2.3446475209 \tValid loss=2.3446122805\tTime=92.90s\n",
      "Best model: Model at epoch 35 with valid loss 2.3446122805\n",
      "Epoch 36/50 \tTrain loss=2.3446477218 \tValid loss=2.3446105321\tTime=92.97s\n",
      "Best model: Model at epoch 36 with valid loss 2.3446105321\n",
      "Epoch 37/50 \tTrain loss=2.3446478202 \tValid loss=2.3446098963\tTime=93.20s\n",
      "Best model: Model at epoch 37 with valid loss 2.3446098963\n",
      "Epoch 38/50 \tTrain loss=2.3446478834 \tValid loss=2.3446098566\tTime=94.85s\n",
      "Best model: Model at epoch 38 with valid loss 2.3446098566\n",
      "Epoch 39/50 \tTrain loss=2.3446480189 \tValid loss=2.3446098566\tTime=93.56s\n",
      "Epoch 40/50 \tTrain loss=2.3446481293 \tValid loss=2.3446101745\tTime=92.89s\n",
      "Epoch 41/50 \tTrain loss=2.3446482831 \tValid loss=2.3446107308\tTime=93.23s\n",
      "Epoch 42/50 \tTrain loss=2.3446484653 \tValid loss=2.3446113269\tTime=93.27s\n",
      "Epoch 43/50 \tTrain loss=2.3446485632 \tValid loss=2.3446117640\tTime=93.18s\n",
      "Epoch 44/50 \tTrain loss=2.3446488238 \tValid loss=2.3446110090\tTime=92.75s\n",
      "Epoch 45/50 \tTrain loss=2.3446489521 \tValid loss=2.3446111679\tTime=93.16s\n",
      "Epoch 46/50 \tTrain loss=2.3446490463 \tValid loss=2.3446115255\tTime=93.29s\n",
      "Epoch 47/50 \tTrain loss=2.3446491798 \tValid loss=2.3446108898\tTime=94.72s\n",
      "Epoch 48/50 \tTrain loss=2.3446492100 \tValid loss=2.3446100156\tTime=92.79s\n",
      "Epoch 49/50 \tTrain loss=2.3446492165 \tValid loss=2.3446098566\tTime=93.17s\n",
      "Epoch 50/50 \tTrain loss=2.3446492013 \tValid loss=2.3446100553\tTime=93.05s\n",
      "--\n",
      "Best model: Model at epoch 38 with valid loss 2.3446098566\n"
     ]
    }
   ],
   "source": [
    "min_valid_loss = np.inf\n",
    "best_model = None\n",
    "best_model_epoch = -2\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        train_loss = 0.0\n",
    "        model.train()\n",
    "        for loader in train_loader:\n",
    "            for i, (mfcc, vrm_params) in enumerate(loader):\n",
    "                mfcc = mfcc.to(device=device).reshape(-1, 1, lstm_input_size)\n",
    "                vrm_params = vrm_params.to(device=device).reshape(-1, 1, output_dim)\n",
    "        \n",
    "                pred = model(mfcc)\n",
    "        \n",
    "                loss = loss_fn(pred, vrm_params)\n",
    "        \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "            \n",
    "        train_loss /= sum([len(loader) for loader in train_loader])\n",
    "            \n",
    "        valid_loss = 0.0\n",
    "        model.eval()\n",
    "        for i, (mfcc, vrm_params) in enumerate(valid_loader):\n",
    "            mfcc = mfcc.to(device=device).reshape(-1, 1, lstm_input_size)\n",
    "            vrm_params = vrm_params.to(device=device).reshape(-1, 1, output_dim)\n",
    "            \n",
    "            pred = model(mfcc)\n",
    "            \n",
    "            loss = loss_fn(pred, vrm_params)\n",
    "            valid_loss += loss.item()\n",
    "    \n",
    "        valid_loss /= len(valid_loader)\n",
    "    \n",
    "        print('Epoch {}/{} \\tTrain loss={:.10f} \\tValid loss={:.10f}\\tTime={:.2f}s'.format(epoch + 1, n_epochs, train_loss, valid_loss, time.time() - start_time))\n",
    "    \n",
    "        if ((epoch+1) % save_interval == 0):\n",
    "            save_model(model, 'sample-model_' + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%SZ\") + '_EPOCH' + str(epoch+1), train_file, batch_size, epoch+1, lr, hidden_state_size, num_sequence_layers, weights, valid_loss)\n",
    "        \n",
    "        if min_valid_loss > valid_loss:\n",
    "            min_valid_loss = valid_loss\n",
    "            best_model = model.state_dict()\n",
    "            best_model_epoch = epoch\n",
    "    \n",
    "            print('Best model: Model at epoch {} with valid loss {:.10f}'.format(best_model_epoch+1, min_valid_loss))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "print('--\\nBest model: Model at epoch {} with valid loss {:.10f}'.format(best_model_epoch+1, min_valid_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_file = 'sample-model_2023-08-17_21-31-49Z_EPOCH15'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_file = 'model_' + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%SZ\")\n",
    "# Save latest model\n",
    "save_model(model, weights_file + '_LATEST', str(train_file), batch_size, n_epochs, lr, hidden_state_size, num_sequence_layers, weights, valid_loss)\n",
    "# Save best model\n",
    "save_model_state(best_model, weights_file + '_BEST', str(train_file), batch_size, best_model_epoch+1, lr, hidden_state_size, num_sequence_layers, weights, min_valid_loss)\n",
    "weights_file += '_BEST'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTM(\n",
       "  (dropout): Dropout(p=0.01, inplace=False)\n",
       "  (lstm): LSTM(2496, 512, num_layers=2, batch_first=True, dropout=0.01, bidirectional=True)\n",
       "  (mdn): MDN(\n",
       "    (pi): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=1, bias=True)\n",
       "      (1): Softmax(dim=1)\n",
       "    )\n",
       "    (sigma): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (mu): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  )\n",
       "  (linear_hidden): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (energy): Linear(in_features=1536, out_features=1, bias=True)\n",
       "  (softmax): Softmax(dim=0)\n",
       "  (relu): ReLU()\n",
       "  (fc): Linear(in_features=2048, out_features=384, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BiLSTM(lstm_input_size, hidden_state_size, batch_size, output_dim, num_sequence_layers, dropoff)\n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load('models/{}.pt'.format(weights_file), map_location=device))\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTM(\n",
       "  (dropout): Dropout(p=0.01, inplace=False)\n",
       "  (lstm): LSTM(2496, 512, num_layers=2, batch_first=True, dropout=0.01, bidirectional=True)\n",
       "  (mdn): MDN(\n",
       "    (pi): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=1, bias=True)\n",
       "      (1): Softmax(dim=1)\n",
       "    )\n",
       "    (sigma): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (mu): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  )\n",
       "  (linear_hidden): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (energy): Linear(in_features=1536, out_features=1, bias=True)\n",
       "  (softmax): Softmax(dim=0)\n",
       "  (relu): ReLU()\n",
       "  (fc): Linear(in_features=2048, out_features=384, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.0818010792\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([537, 384])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds = torch.zeros(output_dim * len(test))\n",
    "test_actual = torch.zeros(output_dim * len(test))\n",
    "\n",
    "for i, (mfcc, vrm_params) in enumerate(test_loader):\n",
    "    mfcc = mfcc.to(device=device).reshape(-1, 1, lstm_input_size)\n",
    "    if not test.audio_only:\n",
    "        vrm_params = vrm_params.cpu().reshape(-1)\n",
    "    y_pred = model(mfcc).cpu().detach().reshape(-1)\n",
    "    test_preds[i*batch_size*output_dim:(i+1)*batch_size*output_dim] = y_pred\n",
    "    if not test.audio_only:\n",
    "        test_actual[i*batch_size*output_dim:(i+1)*batch_size*output_dim] = vrm_params\n",
    "\n",
    "if not test.audio_only:\n",
    "    rmse = torch.sqrt(torch.nn.functional.mse_loss(test_preds, test_actual))\n",
    "    print('RMSE: {:.10f}'.format(rmse))\n",
    "        \n",
    "test_preds = test_preds.reshape(len(test), output_dim)\n",
    "test_actual = test_actual.reshape(len(test), output_dim)\n",
    "\n",
    "test_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1744,  0.0896,  0.0286,  ..., -0.0024,  0.0024, -0.0033],\n",
       "        [ 0.1742,  0.0899,  0.0285,  ..., -0.0023,  0.0023, -0.0032],\n",
       "        [ 0.1744,  0.0894,  0.0286,  ..., -0.0022,  0.0022, -0.0032],\n",
       "        ...,\n",
       "        [ 0.1440,  0.0977,  0.0135,  ...,  0.0007, -0.0011,  0.0026],\n",
       "        [ 0.1438,  0.0982,  0.0133,  ...,  0.0007, -0.0011,  0.0027],\n",
       "        [ 0.1438,  0.0979,  0.0134,  ...,  0.0008, -0.0011,  0.0026]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTION_DATA_PATH = './sampledata/Prediction'\n",
    "\n",
    "def to_csv(test_preds, name='prediction'):\n",
    "    test_preds = train[0].scaler.inverse_transform(test_preds)\n",
    "    \n",
    "    blendshape_params = pd.DataFrame(test_preds[:,:len(EXPR_BS_PARAMS)])\n",
    "    blendshape_params.columns = EXPR_BS_PARAMS\n",
    "    # maxexp = blendshape_params.idxmax(axis=1)\n",
    "    # for i in range(len(blendshape_params)):\n",
    "    #     blendshape_params.iloc[i, :] = 0\n",
    "    #     blendshape_params.loc[i, maxexp.iloc[i]] = 1\n",
    "    # Ignore expression parameters\n",
    "    for param in BLENDSHAPE_PARAMS:\n",
    "        if param not in EXPR_BS_PARAMS:\n",
    "            blendshape_params[param] = 0\n",
    "    blendshape_params.loc[:, 'Neutral'] = 1\n",
    "    blendshape_params = blendshape_params.reindex(BLENDSHAPE_PARAMS, axis=1)\n",
    "    \n",
    "    bone_params = pd.DataFrame(test_preds[:,len(EXPR_BS_PARAMS):])\n",
    "    bone_params.columns = [bone + t for t in ('PosX', 'PosY', 'PosZ', 'RotX', 'RotY', 'RotZ', 'RotW') for bone in BONE_PARAMS]\n",
    "    \n",
    "    STATIC_FRAME = test.STATIC_FRAME\n",
    "    \n",
    "    time_column = pd.DataFrame({'Time': [float(i)*STATIC_FRAME for i in range(len(test_preds))]})\n",
    "    \n",
    "    blendshape_params = pd.concat([time_column, blendshape_params], axis=1)\n",
    "    bone_params = pd.concat([time_column, bone_params], axis=1)\n",
    "    \n",
    "    blendshape_params.to_csv(PREDICTION_DATA_PATH + '/Blendshapes/' + name + '.csv', index=False)\n",
    "    bone_params.to_csv(PREDICTION_DATA_PATH + '/Bones/' + name + '.csv', index=False)\n",
    "    \n",
    "    shutil.copyfile(test.DATA_PATH + '/Audio/' + test.name + '.wav', PREDICTION_DATA_PATH + '/Audio/' + name + '.wav')\n",
    "\n",
    "datestring = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%SZ\")\n",
    "to_csv(test_preds, 'prediction-' + datestring)\n",
    "if not test.audio_only:\n",
    "    to_csv(test_actual, 'actual-' + datestring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
