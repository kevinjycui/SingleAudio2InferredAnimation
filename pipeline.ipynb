{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cpu\n",
      "2.0.2+cpu\n"
     ]
    },
    {
     "ename": "ParameterError",
     "evalue": "width=187 must be at least 1 and at most (data.shape[-1] - 1) // 2=1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParameterError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 199\u001b[0m\n\u001b[0;32m    196\u001b[0m model \u001b[39m=\u001b[39m Bi_RNN(lstm_input_size, hidden_state_size, batch_size, output_dim, num_sequence_layers, rnn_type)\n\u001b[0;32m    197\u001b[0m model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m--> 199\u001b[0m \u001b[39mprint\u001b[39m(train[\u001b[39m0\u001b[39;49m])\n",
      "Cell \u001b[1;32mIn[4], line 148\u001b[0m, in \u001b[0;36mVRMParamsDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    145\u001b[0m SPEECH_WAVEFORM \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(SPEECH_WAVEFORM, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[0;32m    147\u001b[0m S_full, phase \u001b[39m=\u001b[39m librosa\u001b[39m.\u001b[39mmagphase(librosa\u001b[39m.\u001b[39mstft(np\u001b[39m.\u001b[39marray(SPEECH_WAVEFORM)))\n\u001b[1;32m--> 148\u001b[0m S_filter \u001b[39m=\u001b[39m librosa\u001b[39m.\u001b[39;49mdecompose\u001b[39m.\u001b[39;49mnn_filter(S_full,\n\u001b[0;32m    149\u001b[0m                                aggregate\u001b[39m=\u001b[39;49mnp\u001b[39m.\u001b[39;49mmedian,\n\u001b[0;32m    150\u001b[0m                                metric\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcosine\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m    151\u001b[0m                                width\u001b[39m=\u001b[39;49m\u001b[39mint\u001b[39;49m(librosa\u001b[39m.\u001b[39;49mtime_to_frames(\u001b[39m2\u001b[39;49m, sr\u001b[39m=\u001b[39;49mSAMPLE_RATE)))\n\u001b[0;32m    152\u001b[0m S_filter \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mminimum(S_full, S_filter)\n\u001b[0;32m    153\u001b[0m margin_v \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n",
      "File \u001b[1;32mf:\\Users\\Kevin\\SingleAudio2InferredAnimation\\env\\Lib\\site-packages\\librosa\\decompose.py:542\u001b[0m, in \u001b[0;36mnn_filter\u001b[1;34m(S, rec, aggregate, axis, **kwargs)\u001b[0m\n\u001b[0;32m    540\u001b[0m     kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(kwargs)\n\u001b[0;32m    541\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39msparse\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> 542\u001b[0m     rec_s \u001b[39m=\u001b[39m segment\u001b[39m.\u001b[39;49mrecurrence_matrix(S, axis\u001b[39m=\u001b[39;49maxis, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    543\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m scipy\u001b[39m.\u001b[39msparse\u001b[39m.\u001b[39missparse(rec):\n\u001b[0;32m    544\u001b[0m     rec_s \u001b[39m=\u001b[39m scipy\u001b[39m.\u001b[39msparse\u001b[39m.\u001b[39mcsc_matrix(rec)\n",
      "File \u001b[1;32mf:\\Users\\Kevin\\SingleAudio2InferredAnimation\\env\\Lib\\site-packages\\librosa\\segment.py:593\u001b[0m, in \u001b[0;36mrecurrence_matrix\u001b[1;34m(data, k, width, metric, sym, sparse, mode, bandwidth, self, axis, full)\u001b[0m\n\u001b[0;32m    590\u001b[0m data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mreshape((t, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), order\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mF\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    592\u001b[0m \u001b[39mif\u001b[39;00m width \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m \u001b[39mor\u001b[39;00m width \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m (t \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m--> 593\u001b[0m     \u001b[39mraise\u001b[39;00m ParameterError(\n\u001b[0;32m    594\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mwidth=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m must be at least 1 and at most (data.shape[\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m] - 1) // 2=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    595\u001b[0m             width, axis, (t \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m2\u001b[39m\n\u001b[0;32m    596\u001b[0m         )\n\u001b[0;32m    597\u001b[0m     )\n\u001b[0;32m    599\u001b[0m \u001b[39mif\u001b[39;00m mode \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mconnectivity\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mdistance\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39maffinity\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m    600\u001b[0m     \u001b[39mraise\u001b[39;00m ParameterError(\n\u001b[0;32m    601\u001b[0m         (\n\u001b[0;32m    602\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid mode=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mmode\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. Must be one of \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    603\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m[\u001b[39m\u001b[39m'\u001b[39m\u001b[39mconnectivity\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mdistance\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39maffinity\u001b[39m\u001b[39m'\u001b[39m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    604\u001b[0m         )\n\u001b[0;32m    605\u001b[0m     )\n",
      "\u001b[1;31mParameterError\u001b[0m: width=187 must be at least 1 and at most (data.shape[-1] - 1) // 2=1"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pydub import AudioSegment\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "import torchaudio.functional as F\n",
    "import torchaudio.transforms as T\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchaudio.__version__)\n",
    "\n",
    "import librosa\n",
    "\n",
    "from IPython.display import Audio\n",
    "from torchaudio.utils import download_asset\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()   # interactive mode\n",
    "\n",
    "TRAINING_DATA_PATH = \"C:/Users/Kevin/AppData/LocalLow/kevinjycui/Training\"\n",
    "TESTING_DATA_PATH = \"C:/Users/Kevin/AppData/LocalLow/kevinjycui/Testing\"\n",
    "\n",
    "BLENDSHAPE_PARAMS = [\"A\", \"Angry\", \"Blink\", \"Blink_L\", \"Blink_R\", \"E\", \"Fun\", \"I\", \"Joy\", \"LookDown\", \"LookLeft\", \"LookRight\", \"LookUp\", \"Neutral\", \"O\", \"Sorrow\", \"Surprised\", \"U\"]\n",
    "BONE_PARAMS = [\"Chest\", \"Head\", \"Hips\", \"LeftEye\", \"LeftFoot\", \"LeftHand\", \"LeftIndexDistal\", \"LeftIndexIntermediate\", \"LeftIndexProximal\", \"LeftLittleDistal\", \"LeftLittleIntermediate\", \"LeftLittleProximal\", \"LeftLowerArm\", \"LeftLowerLeg\", \"LeftMiddleDistal\", \"LeftMiddleIntermediate\", \"LeftMiddleProximal\", \"LeftRingDistal\", \"LeftRingIntermediate\", \"LeftRingProximal\", \"LeftShoulder\", \"LeftThumbDistal\", \"LeftThumbIntermediate\", \"LeftThumbProximal\", \"LeftToes\", \"LeftUpperArm\", \"LeftUpperLeg\", \"Neck\", \"RightEye\", \"RightFoot\", \"RightHand\", \"RightIndexDistal\", \"RightIndexIntermediate\", \"RightIndexProximal\", \"RightLittleDistal\", \"RightLittleIntermediate\", \"RightLittleProximal\", \"RightLowerArm\", \"RightLowerLeg\", \"RightMiddleDistal\", \"RightMiddleIntermediate\", \"RightMiddleProximal\", \"RightRingDistal\", \"RightRingIntermediate\", \"RightRingProximal\", \"RightShoulder\", \"RightThumbDistal\", \"RightThumbIntermediate\", \"RightThumbProximal\", \"RightToes\", \"RightUpperArm\", \"RightUpperLeg\", \"Spine\", \"UpperChest\"]\n",
    "\n",
    "class Bi_RNN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, batch_size, output_dim=11, num_layers=2, rnn_type='LSTM'):\n",
    "        super(Bi_RNN, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        #Define the initial linear hidden layer\n",
    "        self.init_linear = nn.Linear(self.input_dim, self.input_dim)\n",
    "\n",
    "        # Define the LSTM layer\n",
    "        self.lstm = eval('nn.' + rnn_type)(self.input_dim, self.hidden_dim, self.num_layers, batch_first=True, bidirectional=True)\n",
    "\n",
    "        # Define the output layer\n",
    "        self.linear = nn.Linear(self.hidden_dim * 2, output_dim)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # This is what we'll initialise our hidden state as\n",
    "        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))\n",
    "\n",
    "    def forward(self, input):\n",
    "        #Forward pass through initial hidden layer\n",
    "        linear_input = self.init_linear(input)\n",
    "\n",
    "        # Forward pass through LSTM layer\n",
    "        # shape of lstm_out: [batch_size, input_size ,hidden_dim]\n",
    "        # shape of self.hidden: (a, b), where a and b both\n",
    "        # have shape (batch_size, num_layers, hidden_dim).\n",
    "        lstm_out, self.hidden = self.lstm(linear_input)\n",
    "\n",
    "        # Can pass on the entirety of lstm_out to the next layer if it is a seq2seq prediction\n",
    "        y_pred = self.linear(lstm_out)\n",
    "        return y_pred\n",
    "\n",
    "def plot_waveform(waveform, sr, title=\"Waveform\"):\n",
    "    waveform = waveform.numpy()\n",
    "\n",
    "    num_channels, num_frames = waveform.shape\n",
    "    time_axis = torch.arange(0, num_frames) / sr\n",
    "\n",
    "    figure, axes = plt.subplots(num_channels, 1)\n",
    "    if num_channels == 1:\n",
    "        axes.plot(time_axis, waveform[0], linewidth=1)\n",
    "        axes.grid(True)\n",
    "    else:\n",
    "        for axis in axes:\n",
    "            axis.plot(time_axis, waveform[0], linewidth=1)\n",
    "            axis.grid(True)\n",
    "    figure.suptitle(title)\n",
    "\n",
    "def plot_spectrogram(specgram, title=None, ylabel=\"freq_bin\"):\n",
    "    fig, axs = plt.subplots(1, 1)\n",
    "    axs.set_title(title or \"Spectrogram (db)\")\n",
    "    axs.set_ylabel(ylabel)\n",
    "    axs.set_xlabel(\"frame\")\n",
    "    im = axs.imshow(librosa.power_to_db(specgram), origin=\"lower\", aspect=\"auto\")\n",
    "    fig.colorbar(im, ax=axs)\n",
    "\n",
    "class VRMParamsDataset(Dataset):\n",
    "    \"\"\" VRM Parameter Dataset \"\"\"\n",
    "\n",
    "    def __init__(self, filename, DATA_PATH=TRAINING_DATA_PATH, transform=None):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            filename (string): Path to wav/csv files.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.name = Path(filename).stem\n",
    "        self.DATA_PATH = DATA_PATH\n",
    "        self.blendshapes_frame = pd.read_csv(DATA_PATH + \"/Blendshapes/\" + self.name + \".csv\")\n",
    "        self.bones_frame = pd.read_csv(DATA_PATH + \"/Bones/\" + self.name + \".csv\")\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.blendshapes_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        time_window = self.blendshapes_frame.iloc[idx, 0]\n",
    "\n",
    "        blendshape_params = self.blendshapes_frame.iloc[idx, 1:]\n",
    "        blendshape_params = np.asarray(blendshape_params)\n",
    "        blendshape_params = blendshape_params.astype('float')\n",
    "\n",
    "        assert time_window == self.bones_frame.iloc[idx, 0]\n",
    "\n",
    "        bone_params = self.bones_frame.iloc[idx, 1:]\n",
    "        bone_params = np.asarray(bone_params)\n",
    "        bone_params = bone_params.astype('float')\n",
    "\n",
    "        audio_path = self.DATA_PATH + f\"/AudioIntervals/{self.name}/{idx}.wav\"\n",
    "\n",
    "        SPEECH_WAVEFORM, SAMPLE_RATE = torchaudio.load(audio_path)\n",
    "        SPEECH_WAVEFORM = torch.mean(SPEECH_WAVEFORM, dim=0).unsqueeze(0)\n",
    "\n",
    "        def mfcc_transform(n_fft=2048, win_length=None, hop_length=512, n_mels=256, n_mfcc=256):\n",
    "            return T.MFCC(\n",
    "                sample_rate=SAMPLE_RATE,\n",
    "                n_mfcc=n_mfcc,\n",
    "                melkwargs={\n",
    "                    \"n_fft\": n_fft,\n",
    "                    \"n_mels\": n_mels,\n",
    "                    \"hop_length\": hop_length,\n",
    "                    \"mel_scale\": \"htk\",\n",
    "                },\n",
    "            )\n",
    "\n",
    "        mfcc = mfcc_transform()(SPEECH_WAVEFORM)\n",
    "\n",
    "        return mfcc, np.concatenate((blendshape_params, bone_params))\n",
    "\n",
    "n_epochs = 2\n",
    "lr = 0.01\n",
    "n_folds = 5\n",
    "lstm_input_size = 1\n",
    "hidden_state_size = 30\n",
    "batch_size = 30\n",
    "num_sequence_layers = 2\n",
    "output_dim = 11\n",
    "num_time_steps = 4000\n",
    "rnn_type = 'LSTM'\n",
    "print_interval = 3000\n",
    "\n",
    "train = VRMParamsDataset(\"7-5-2023 12-17-53 AM\", TRAINING_DATA_PATH)\n",
    "test = VRMParamsDataset(\"7-4-2023 9-54-11 PM\", TESTING_DATA_PATH)\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model = Bi_RNN(lstm_input_size, hidden_state_size, batch_size, output_dim, num_sequence_layers, rnn_type)\n",
    "model = model.to(device)\n",
    "\n",
    "print(train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParameterError",
     "evalue": "Audio data must be of type numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParameterError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(train[\u001b[39m0\u001b[39;49m])\n",
      "Cell \u001b[1;32mIn[1], line 147\u001b[0m, in \u001b[0;36mVRMParamsDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    144\u001b[0m SPEECH_WAVEFORM, SAMPLE_RATE \u001b[39m=\u001b[39m torchaudio\u001b[39m.\u001b[39mload(audio_path)\n\u001b[0;32m    145\u001b[0m SPEECH_WAVEFORM \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(SPEECH_WAVEFORM, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m--> 147\u001b[0m S_full, phase \u001b[39m=\u001b[39m librosa\u001b[39m.\u001b[39mmagphase(librosa\u001b[39m.\u001b[39;49mstft(SPEECH_WAVEFORM))\n\u001b[0;32m    148\u001b[0m S_filter \u001b[39m=\u001b[39m librosa\u001b[39m.\u001b[39mdecompose\u001b[39m.\u001b[39mnn_filter(S_full,\n\u001b[0;32m    149\u001b[0m                                aggregate\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mmedian,\n\u001b[0;32m    150\u001b[0m                                metric\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcosine\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m    151\u001b[0m                                width\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m(librosa\u001b[39m.\u001b[39mtime_to_frames(\u001b[39m2\u001b[39m, sr\u001b[39m=\u001b[39mSAMPLE_RATE)))\n\u001b[0;32m    152\u001b[0m S_filter \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mminimum(S_full, S_filter)\n",
      "File \u001b[1;32mf:\\Users\\Kevin\\SingleAudio2InferredAnimation\\env\\Lib\\site-packages\\librosa\\core\\spectrum.py:229\u001b[0m, in \u001b[0;36mstft\u001b[1;34m(y, n_fft, hop_length, win_length, window, center, dtype, pad_mode, out)\u001b[0m\n\u001b[0;32m    226\u001b[0m     \u001b[39mraise\u001b[39;00m ParameterError(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhop_length=\u001b[39m\u001b[39m{\u001b[39;00mhop_length\u001b[39m}\u001b[39;00m\u001b[39m must be a positive integer\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    228\u001b[0m \u001b[39m# Check audio is valid\u001b[39;00m\n\u001b[1;32m--> 229\u001b[0m util\u001b[39m.\u001b[39;49mvalid_audio(y, mono\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    231\u001b[0m fft_window \u001b[39m=\u001b[39m get_window(window, win_length, fftbins\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    233\u001b[0m \u001b[39m# Pad the window out to n_fft size\u001b[39;00m\n",
      "File \u001b[1;32mf:\\Users\\Kevin\\SingleAudio2InferredAnimation\\env\\Lib\\site-packages\\librosa\\util\\utils.py:297\u001b[0m, in \u001b[0;36mvalid_audio\u001b[1;34m(y, mono)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Determine whether a variable contains valid audio data.\u001b[39;00m\n\u001b[0;32m    242\u001b[0m \n\u001b[0;32m    243\u001b[0m \u001b[39mThe following conditions must be satisfied:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[39mnumpy.float32\u001b[39;00m\n\u001b[0;32m    294\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    296\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(y, np\u001b[39m.\u001b[39mndarray):\n\u001b[1;32m--> 297\u001b[0m     \u001b[39mraise\u001b[39;00m ParameterError(\u001b[39m\"\u001b[39m\u001b[39mAudio data must be of type numpy.ndarray\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    299\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39missubdtype(y\u001b[39m.\u001b[39mdtype, np\u001b[39m.\u001b[39mfloating):\n\u001b[0;32m    300\u001b[0m     \u001b[39mraise\u001b[39;00m ParameterError(\u001b[39m\"\u001b[39m\u001b[39mAudio data must be floating-point\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mParameterError\u001b[0m: Audio data must be of type numpy.ndarray"
     ]
    }
   ],
   "source": [
    "print(train[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i, (name, label) in enumerate(train):\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_correct = 0\n",
    "num_samples = len(test)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    pass\n",
    "\n",
    "print(f\"Accuracy: {num_correct / num_samples * 100:.4f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
